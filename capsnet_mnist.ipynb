{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "from sklearn.datasets import fetch_mldata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "TRAINDATA = mnist['data'][:60000]/255.0\n",
    "TESTDATA = mnist['data'][60000:]/255.0\n",
    "TRAINLABELS = mnist['target'][:60000]\n",
    "TESTLABELS = mnist['target'][60000:]\n",
    "\n",
    "# get validation set from training\n",
    "listIdxs = range(TRAINLABELS.shape[0])\n",
    "np.random.shuffle(listIdxs)\n",
    "np.random.shuffle(listIdxs)\n",
    "VALDATA = TRAINDATA[listIdxs[:6000]]\n",
    "VALLABELS = TRAINLABELS[listIdxs[:6000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Overarching Model\n",
    "'''\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, routing_iterations, \n",
    "                 preCapDepth, numCaps, numPerCap, capFilterSize,\n",
    "                c1kernelSizes, c1kernelStrides, c1numMaps, n_classes=10):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.preCapDepth = preCapDepth\n",
    "        self.numCaps = numCaps\n",
    "        self.numPerCap = numPerCap\n",
    "        self.c1kernelSizes = c1kernelSizes\n",
    "        self.c1kernelStrides = c1kernelStrides\n",
    "        self.c1numMaps = c1numMaps\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.conv1 = PrimaryConvs(self.c1kernelSizes, self.c1kernelStrides,self.c1numMaps)\n",
    "        \n",
    "        #self.primaryCaps = PrimaryCapsLayer(self.preCapDepth, self.m=numCaps, self.numPerCap, kernel_size=9, stride=2)  # outputs: normalized sheets\n",
    "        self.primaryCaps = PrimaryCapsLayer(self.preCapDepth, self.numCaps, self.numPerCap)\n",
    "        \n",
    "        self.num_primaryCaps = numCaps * (capFilterSize**2)\n",
    "        routing_module = AgreementRouting(self.num_primaryCaps, n_classes, routing_iterations)\n",
    "        self.digitCaps = CapsLayer(self.num_primaryCaps, self.numPerCap, n_classes, 16, routing_module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.primaryCaps(x) # output sheet\n",
    "        x = self.digitCaps(x) # output score matrix (before final flat)\n",
    "        probs = x.pow(2).sum(dim=2).sqrt()\n",
    "        return x, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: FM after 1st convolution\n",
    "Output: Normalized sheet\n",
    "'''\n",
    "class PrimaryCapsLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_caps, output_dim):\n",
    "        super(PrimaryCapsLayer, self).__init__()\n",
    "        #self.conv = nn.Conv2d(input_channels, output_caps * output_dim, kernel_size=kernel_size, stride=stride)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_caps = output_caps\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        #out = self.conv(input)\n",
    "        out = input\n",
    "        N, C, H, W = out.size()\n",
    "        out = out.view(N, self.output_caps, self.output_dim, H, W) # splitting into capsules\n",
    "\n",
    "        # will output N x OUT_CAPS x OUT_DIM\n",
    "        out = out.permute(0, 1, 3, 4, 2).contiguous() \n",
    "        out = out.view(out.size(0), -1, out.size(4)) # flat N*sheet\n",
    "        out = squash(out) # normalize\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "Input: FM after 1st convolution\n",
    "Output: Normalized sheet\n",
    "'''\n",
    "class PrimaryConvs(nn.Module):\n",
    "    def __init__(self, c1kernelSizes, c1kernelStrides, c1numMaps):\n",
    "        super(PrimaryConvs, self).__init__()\n",
    "        self.c1kernelSizes = c1kernelSizes\n",
    "        self.c1kernelStrides = c1kernelStrides\n",
    "        self.c1numMaps = c1numMaps\n",
    "        self.numLayers = len(c1numMaps)\n",
    "        \n",
    "        self.conv1 = None\n",
    "        if self.numLayers >= 1:\n",
    "            self.conv1 = nn.Conv2d(1, self.c1numMaps[0], \n",
    "                                   kernel_size=self.c1kernelSizes[0], \n",
    "                                   stride=self.c1kernelStrides[0])\n",
    "        self.conv2 = None\n",
    "        if self.numLayers >= 2:\n",
    "            self.conv2 = nn.Conv2d(self.c1numMaps[0], self.c1numMaps[1], \n",
    "                                   kernel_size=self.c1kernelSizes[1], \n",
    "                                   stride=self.c1kernelStrides[1])\n",
    "        self.conv3 = None\n",
    "        if self.numLayers >= 3:\n",
    "            self.conv3 = nn.Conv2d(self.c1numMaps[1], self.c1numMaps[2], \n",
    "                                   kernel_size=self.c1kernelSizes[2], \n",
    "                                   stride=self.c1kernelStrides[2])\n",
    "        self.conv4 = None\n",
    "        if self.numLayers >= 4:\n",
    "            self.conv4 = nn.Conv2d(self.c1numMaps[2], self.c1numMaps[3], \n",
    "                                   kernel_size=self.c1kernelSizes[3], \n",
    "                                   stride=self.c1kernelStrides[3])\n",
    "    def forward(self, x):\n",
    "        if self.conv1:\n",
    "            x = self.conv1(x)\n",
    "            if self.numLayers != 1:\n",
    "                x = F.relu(x)\n",
    "        if self.conv2:\n",
    "            x = self.conv2(x)\n",
    "            if self.numLayers != 2:\n",
    "                x = F.relu(x)\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "            if self.numLayers != 3:\n",
    "                x = F.relu(x)\n",
    "        if self.conv4:\n",
    "            x = self.conv4(x)\n",
    "            if self.numLayers != 4:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    lengths2 = x.pow(2).sum(dim=2)\n",
    "    lengths = lengths2.sqrt()\n",
    "    x = x * (lengths2 / (1 + lengths2) / lengths).view(x.size(0), x.size(1), 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class AgreementRouting(nn.Module):\n",
    "    def __init__(self, input_caps, output_caps, n_iterations):\n",
    "        super(AgreementRouting, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.b = nn.Parameter(torch.zeros((input_caps, output_caps)))\n",
    "\n",
    "    def forward(self, u_predict):\n",
    "        batch_size, input_caps, output_caps, output_dim = u_predict.size()\n",
    "        \n",
    "        c = F.softmax(self.b)\n",
    "        s = (c.unsqueeze(2) * u_predict).sum(dim=1)\n",
    "        v = squash(s)\n",
    "\n",
    "        if self.n_iterations > 0:\n",
    "            b_batch = self.b.expand((batch_size, input_caps, output_caps))\n",
    "            for r in range(self.n_iterations):\n",
    "                v = v.unsqueeze(1)\n",
    "                b_batch = b_batch + (u_predict * v).sum(-1)\n",
    "\n",
    "                c = F.softmax(b_batch.view(-1, output_caps)).view(-1, input_caps, output_caps, 1)\n",
    "                s = (c * u_predict).sum(dim=1)\n",
    "                v = squash(s)\n",
    "        return v\n",
    "\n",
    "\n",
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, input_caps, input_dim, output_caps, output_dim, routing_module):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_caps = input_caps\n",
    "        self.output_dim = output_dim\n",
    "        self.output_caps = output_caps\n",
    "        self.weights = nn.Parameter(torch.Tensor(input_caps, input_dim, output_caps * output_dim))\n",
    "        self.routing_module = routing_module\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.input_caps)\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, caps_output):\n",
    "        caps_output = caps_output.unsqueeze(2)\n",
    "        u_predict = caps_output.matmul(self.weights)\n",
    "        u_predict = u_predict.view(u_predict.size(0), self.input_caps, self.output_caps, self.output_dim)\n",
    "        v = self.routing_module(u_predict)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruction Stuff \n",
    "class ReconstructionNet(nn.Module):\n",
    "    def __init__(self, n_dim=16, n_classes=10):\n",
    "        super(ReconstructionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_dim * n_classes, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 784)\n",
    "        self.n_dim = n_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        mask = Variable(torch.zeros((x.size()[0], self.n_classes)), requires_grad=False)\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask.scatter_(1, target.view(-1, 1), 1.)\n",
    "        mask = mask.unsqueeze(2)\n",
    "        x = x * mask\n",
    "        x = x.view(-1, self.n_dim * self.n_classes)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CapsNetWithReconstruction(nn.Module):\n",
    "    def __init__(self, capsnet, reconstruction_net):\n",
    "        super(CapsNetWithReconstruction, self).__init__()\n",
    "        self.capsnet = capsnet\n",
    "        self.reconstruction_net = reconstruction_net\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x, probs = self.capsnet(x)\n",
    "        reconstruction = self.reconstruction_net(x, target)\n",
    "        return reconstruction, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = [torch.DoubleTensor(d).view(28,28) for d in data]\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "TRAINDATASET = GeneralDataset(TRAINDATA,TRAINLABELS)\n",
    "VALDATASET = GeneralDataset(VALDATA,VALLABELS)\n",
    "TESTDATASET = GeneralDataset(TESTDATA,TESTLABELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Loss Functions\n",
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, m_pos, m_neg, lambda_):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, lengths, targets, size_average=True):\n",
    "        t = torch.zeros(lengths.size()).long()\n",
    "        if targets.is_cuda:\n",
    "            t = t.cuda()\n",
    "        t = t.scatter_(1, targets.data.view(-1, 1), 1)\n",
    "        targets = Variable(t)\n",
    "        losses = targets.float() * F.relu(self.m_pos - lengths).pow(2) + \\\n",
    "                 self.lambda_ * (1. - targets.float()) * F.relu(lengths - self.m_neg).pow(2)\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "### save model\n",
    "def saveModel(model,name,state_dict=False):\n",
    "    if state_dict:\n",
    "        torch.save(model.state_dict(),name)\n",
    "    else:\n",
    "        torch.save(model,name)\n",
    "\n",
    "\n",
    "### rotation transform class\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, degrees_tup):\n",
    "        self.dt = [0]*6\n",
    "        self.dt[degrees_tup] = 1\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if self.dt[0]:\n",
    "            return sample\n",
    "        sample = np.asarray(sample)\n",
    "       \n",
    "        if self.dt[0]:\n",
    "            sample = rotate(randint(0, 1),sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[1]:\n",
    "            sample = rotate(randint(0, 1)*2,sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[2]:\n",
    "            sample = rotate(randint(0, 1)*3,sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[3]:\n",
    "            sample = rotate(randint(1, 3),sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[4]:\n",
    "            sample = rotate(randint(0, 3),sample)\n",
    "            return np2PIL(sample)\n",
    "def np2PIL(data):\n",
    "    return Image.fromarray(data)\n",
    "def rotate(i,data):\n",
    "    if i == 0:\n",
    "        return data\n",
    "    elif i == 1:\n",
    "        return np.rot90(data)\n",
    "    elif i == 2:\n",
    "        return np.rot90(data,2)\n",
    "    elif i == 3:\n",
    "        return np.rot90(data,3)\n",
    "    \n",
    "### Normalize\n",
    "def normalize(tensorData):\n",
    "    return torch.Tensor(tensorData.numpy()/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Epoch,Loss,numStatsPerEpoch,Rec_loss_every=1):\n",
    "    model.train()\n",
    "    statIdx = [len(TRAINLOADER)/numStatsPerEpoch]*numStatsPerEpoch\n",
    "    statIdx = [(i)*a for i,a in enumerate(statIdx)]\n",
    "    for batch_idx, (data, target) in enumerate(TRAINLOADER):\n",
    "        \n",
    "        # get stats\n",
    "        if (batch_idx) in statIdx:\n",
    "            val_loss, val_acc,VN,VD = getLossAccND('Validation',VALLOADER)\n",
    "            test_loss, test_acc,TN,TD = getLossAccND('Test',TESTLOADER)\n",
    "            print\n",
    "            ValLoss.append(val_loss);ValAcc.append(val_acc);\n",
    "            TestLoss.append(test_loss);TestAcc.append(test_acc);\n",
    "    \n",
    "        # train\n",
    "        data = data.view(-1,1,28,28)\n",
    "        data, target = data.float(), target.long()\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        if RECONSTRUCTION:\n",
    "            output, probs = model(data, target)\n",
    "            reconstruction_loss = F.mse_loss(output, data.view(-1, 784))\n",
    "            margin_loss = loss_fn(probs, target)\n",
    "            loss = reconstruction_alpha * reconstruction_loss + margin_loss\n",
    "        else:\n",
    "            output, probs = model(data)\n",
    "            loss = loss_fn(probs, target)\n",
    "        \n",
    "        if batch_idx % Rec_loss_every == 0:\n",
    "            Loss.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INT == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                Epoch, batch_idx * len(data), len(TRAINLOADER.dataset),\n",
    "                       100. * batch_idx / len(TRAINLOADER), loss.data[0]))\n",
    "\n",
    "def getLossAccND(name,dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.view(-1,1,28,28)\n",
    "        data, target = data.float(), target.long()\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        #print data, target\n",
    "        if RECONSTRUCTION:\n",
    "            output, probs = model(data, target)\n",
    "            reconstruction_loss = F.mse_loss(output, data.view(-1, 784), size_average=False).data[0]\n",
    "            loss += loss_fn(probs, target, size_average=False).data[0]\n",
    "            loss += reconstruction_alpha * reconstruction_loss\n",
    "        else:\n",
    "            output, probs = model(data)\n",
    "            loss += loss_fn(probs, target, size_average=False).data[0]\n",
    "\n",
    "        pred = probs.data.max(1, keepdim=True)[1]  # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss /= len(dataloader.dataset)\n",
    "    print('{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        name,loss, correct, len(dataloader.dataset),\n",
    "        100. * correct / len(dataloader.dataset)))\n",
    "    return (loss, 100. * correct / len(dataloader.dataset), correct, len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining paramters, loaders, SGD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training settings\n",
    "# parser = argparse.ArgumentParser(description='CapsNet with MNIST')\n",
    "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "#                     help='input batch size for training (default: 64)')\n",
    "# parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "#                     help='input batch size for testing (default: 1000)')\n",
    "# parser.add_argument('--epochs', type=int, default=250, metavar='N',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                     help='learning rate (default: 0.01)')\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                     help='disables CUDA training')\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                     help='random seed (default: 1)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "# parser.add_argument('--routing_iterations', type=int, default=3)\n",
    "# parser.add_argument('--with_reconstruction', action='store_true', default=False)\n",
    "# args = parser.parse_args()\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "### Model Architecture Params\n",
    "ROUTING_ITERS = 3\n",
    "CONVKERNELSIZES, CONVKERNELSTRIDES, CONVNUMMAPS = \\\n",
    "    [9],[2],[256]\n",
    "PRECAPDEPTH = CONVNUMMAPS[-1]\n",
    "NUMCAPS = 32\n",
    "NUMPERCAP = 8  #Note: precapdepth=numcaps*numpercap\n",
    "CAPFILTERSIZE = 10\n",
    "\n",
    "\n",
    "### Training Params\n",
    "BATCH_SIZE = 200\n",
    "TEST_BATCH_SIZE = 400\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_EPOCH = 3\n",
    "CUDA = torch.cuda.is_available()\n",
    "SEED = 1\n",
    "LOG_INT = 10 # how many batches to wait before logging training status\n",
    "RECONSTRUCTION = False\n",
    "ROTATION_TYPE = 3\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "\n",
    "### Stats\n",
    "NUMSTATSPEREPOCH = 4\n",
    "\n",
    "\n",
    "TRAINLOADER = DataLoader(TRAINDATASET, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, num_workers=4)\n",
    "VALLOADER = DataLoader(VALDATASET, batch_size=TEST_BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=4) \n",
    "TESTLOADER = DataLoader(TESTDATASET, batch_size=TEST_BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=4) \n",
    "\n",
    "model = CapsNet(ROUTING_ITERS,\n",
    "                PRECAPDEPTH, NUMCAPS, NUMPERCAP, CAPFILTERSIZE,\n",
    "                CONVKERNELSIZES,CONVKERNELSTRIDES,CONVNUMMAPS,\n",
    "                n_classes=10)\n",
    "#model = CapsNet( ROUTING_ITERS,PRECAPDEPTH, NUMCAPS, NUMPERCAP, CAPFILTERSIZE,\n",
    "#               CONVKERNELSIZES, CONVKERNELSTRIDES, CONVNUMMAPS )\n",
    "\n",
    "if RECONSTRUCTION:\n",
    "    reconstruction_model = ReconstructionNet(16, 10)\n",
    "    reconstruction_alpha = 0.0005\n",
    "    model = CapsNetWithReconstruction(model, reconstruction_model)\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "loss_fn = MarginLoss(0.9, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.7984, Accuracy: 723/6000 (12%)\n",
      "Test set: Average loss: 0.7982, Accuracy: 1230/10000 (12%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5e32ca541041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTestAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUMSTATSPEREPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainL_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestLoss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# get final training and testing stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-448f14db44e4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(Epoch, Loss, numStatsPerEpoch, Rec_loss_every)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mRec_loss_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mLOG_INT\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mgrad_batch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mgrad_batch2\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbmm\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    582\u001b[0m         output = Variable(self.data.new(self.data.size(0), self.data.size(1),\n\u001b[1;32m    583\u001b[0m                                         batch.data.size(2)))\n\u001b[0;32m--> 584\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_static_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaddbmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_static_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_args\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_batch, batch1, batch2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         return torch.baddbmm(alpha, add_batch, beta,\n\u001b[1;32m    109\u001b[0m                              batch1, batch2, out=output)\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/autograd/_functions/blas.pyc\u001b[0m in \u001b[0;36m_get_output\u001b[0;34m(ctx, arg, inplace)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "### Call training functions \n",
    "TrainLoss = []; TrainL_every = 1; TrN=0; TrD=0\n",
    "ValLoss = []; ValAcc = []; VN=0;VD=0\n",
    "TestLoss = []; TestAcc = []; TN=0; TD=0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch,TrainLoss,NUMSTATSPEREPOCH,TrainL_every)\n",
    "    scheduler.step(TestLoss[-1])\n",
    "# get final training and testing stats\n",
    "ftest_loss, ftest_acc,TN,TD = getLossAccND('Test',TESTLOADER)\n",
    "val_loss, val_acc,VN,VD = getLossAccND('Validation',VALLOADER)\n",
    "ftrain_loss, ftrain_acc,TrN,TrD = getLossAccND('Train',TRAINLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network classes\n",
    "#capsnet = CapsNet(3, 10)\n",
    "#reconstructionnet = ReconstructionNet(16, 10)\n",
    "#model = CapsNetWithReconstruction(capsnet, reconstructionnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.plot(TrainLoss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "print 'Final Training Accuracy: %d/%d (%.2f%%)' % (TrN,TrD,ftrain_acc)\n",
    "print 'Final Training Loss: %.5f' % (ftrain_loss)\n",
    "\n",
    "statsIdx = range(len(ValAcc))\n",
    "statsIdx = [float(c)/NUMSTATSPEREPOCH for c in statsIdx]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(statsIdx,ValLoss, 'b-')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(statsIdx,ValAcc, 'r-')\n",
    "ax2.set_ylabel('Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()\n",
    "plt.title('Validation Loss and Accuracy')\n",
    "plt.show()\n",
    "print 'Final Validation Accuracy: %d/%d (%.2f%%)' % (VN,VD,ValAcc[-1])\n",
    "print 'Final Validation Loss: %.5f' % (ValLoss[-1])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(statsIdx,TestLoss, 'b-')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(statsIdx,TestAcc, 'r-')\n",
    "ax2.set_ylabel('Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()\n",
    "plt.title('Test Loss and Accuracy')\n",
    "plt.show()\n",
    "print 'Final Test Accuracy: %d/%d (%.2f%%)' % (TN,TD,TestAcc[-1])\n",
    "print 'Final Test Loss: %.5f' % (TestLoss[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
