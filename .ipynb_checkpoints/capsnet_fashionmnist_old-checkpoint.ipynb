{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "from sklearn.datasets import fetch_mldata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "TRAINDATA = mnist['data'][:60000]/255.0\n",
    "TESTDATA = mnist['data'][60000:]/255.0\n",
    "TRAINLABELS = mnist['target'][:60000]\n",
    "TESTLABELS = mnist['target'][60000:]\n",
    "\n",
    "# get validation set from training\n",
    "listIdxs = range(TRAINLABELS.shape[0])\n",
    "np.random.shuffle(listIdxs)\n",
    "np.random.shuffle(listIdxs)\n",
    "VALDATA = TRAINDATA[listIdxs[:6000]]\n",
    "VALLABELS = TRAINLABELS[listIdxs[:6000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Overarching Model\n",
    "'''\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, routing_iterations, \n",
    "                 preCapDepth, numCaps, numPerCap, capFilterSize,\n",
    "                c1kernelSizes, c1kernelStrides, c1numMaps, n_classes=10):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.preCapDepth = preCapDepth\n",
    "        self.numCaps = numCaps\n",
    "        self.numPerCap = numPerCap\n",
    "        self.c1kernelSizes = c1kernelSizes\n",
    "        self.c1kernelStrides = c1kernelStrides\n",
    "        self.c1numMaps = c1numMaps\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.conv1 = PrimaryConvs(self.c1kernelSizes, self.c1kernelStrides,self.c1numMaps)\n",
    "        \n",
    "        #self.primaryCaps = PrimaryCapsLayer(self.preCapDepth, self.m=numCaps, self.numPerCap, kernel_size=9, stride=2)  # outputs: normalized sheets\n",
    "        self.primaryCaps = PrimaryCapsLayer(self.preCapDepth, self.numCaps, self.numPerCap)\n",
    "        \n",
    "        self.num_primaryCaps = numCaps * (capFilterSize**2)\n",
    "        routing_module = AgreementRouting(self.num_primaryCaps, n_classes, routing_iterations)\n",
    "        self.digitCaps = CapsLayer(self.num_primaryCaps, self.numPerCap, n_classes, 16, routing_module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.primaryCaps(x) # output sheet\n",
    "        x = self.digitCaps(x) # output score matrix (before final flat)\n",
    "        probs = x.pow(2).sum(dim=2).sqrt()\n",
    "        return x, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: FM after 1st convolution\n",
    "Output: Normalized sheet\n",
    "'''\n",
    "class PrimaryCapsLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_caps, output_dim):\n",
    "        super(PrimaryCapsLayer, self).__init__()\n",
    "        #self.conv = nn.Conv2d(input_channels, output_caps * output_dim, kernel_size=kernel_size, stride=stride)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_caps = output_caps\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        #out = self.conv(input)\n",
    "        out = input\n",
    "        N, C, H, W = out.size()\n",
    "        out = out.view(N, self.output_caps, self.output_dim, H, W) # splitting into capsules\n",
    "\n",
    "        # will output N x OUT_CAPS x OUT_DIM\n",
    "        out = out.permute(0, 1, 3, 4, 2).contiguous() \n",
    "        out = out.view(out.size(0), -1, out.size(4)) # flat N*sheet\n",
    "        out = squash(out) # normalize\n",
    "        return out\n",
    "    \n",
    "'''\n",
    "Input: FM after 1st convolution\n",
    "Output: Normalized sheet\n",
    "'''\n",
    "class PrimaryConvs(nn.Module):\n",
    "    def __init__(self, c1kernelSizes, c1kernelStrides, c1numMaps):\n",
    "        super(PrimaryConvs, self).__init__()\n",
    "        self.c1kernelSizes = c1kernelSizes\n",
    "        self.c1kernelStrides = c1kernelStrides\n",
    "        self.c1numMaps = c1numMaps\n",
    "        self.numLayers = len(c1numMaps)\n",
    "        \n",
    "        self.conv1 = None\n",
    "        if self.numLayers >= 1:\n",
    "            self.conv1 = nn.Conv2d(1, self.c1numMaps[0], \n",
    "                                   kernel_size=self.c1kernelSizes[0], \n",
    "                                   stride=self.c1kernelStrides[0])\n",
    "        self.conv2 = None\n",
    "        if self.numLayers >= 2:\n",
    "            self.conv2 = nn.Conv2d(self.c1numMaps[0], self.c1numMaps[1], \n",
    "                                   kernel_size=self.c1kernelSizes[1], \n",
    "                                   stride=self.c1kernelStrides[1])\n",
    "        self.conv3 = None\n",
    "        if self.numLayers >= 3:\n",
    "            self.conv3 = nn.Conv2d(self.c1numMaps[1], self.c1numMaps[2], \n",
    "                                   kernel_size=self.c1kernelSizes[2], \n",
    "                                   stride=self.c1kernelStrides[2])\n",
    "        self.conv4 = None\n",
    "        if self.numLayers >= 4:\n",
    "            self.conv4 = nn.Conv2d(self.c1numMaps[2], self.c1numMaps[3], \n",
    "                                   kernel_size=self.c1kernelSizes[3], \n",
    "                                   stride=self.c1kernelStrides[3])\n",
    "    def forward(self, x):\n",
    "        if self.conv1:\n",
    "            x = self.conv1(x)\n",
    "            if self.numLayers != 1:\n",
    "                x = F.relu(x)\n",
    "        if self.conv2:\n",
    "            x = self.conv2(x)\n",
    "            if self.numLayers != 2:\n",
    "                x = F.relu(x)\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "            if self.numLayers != 3:\n",
    "                x = F.relu(x)\n",
    "        if self.conv4:\n",
    "            x = self.conv4(x)\n",
    "            if self.numLayers != 4:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    lengths2 = x.pow(2).sum(dim=2)\n",
    "    lengths = lengths2.sqrt()\n",
    "    x = x * (lengths2 / (1 + lengths2) / lengths).view(x.size(0), x.size(1), 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class AgreementRouting(nn.Module):\n",
    "    def __init__(self, input_caps, output_caps, n_iterations):\n",
    "        super(AgreementRouting, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.b = nn.Parameter(torch.zeros((input_caps, output_caps)))\n",
    "\n",
    "    def forward(self, u_predict):\n",
    "        batch_size, input_caps, output_caps, output_dim = u_predict.size()\n",
    "        \n",
    "        c = F.softmax(self.b)\n",
    "        s = (c.unsqueeze(2) * u_predict).sum(dim=1)\n",
    "        v = squash(s)\n",
    "\n",
    "        if self.n_iterations > 0:\n",
    "            b_batch = self.b.expand((batch_size, input_caps, output_caps))\n",
    "            for r in range(self.n_iterations):\n",
    "                v = v.unsqueeze(1)\n",
    "                b_batch = b_batch + (u_predict * v).sum(-1)\n",
    "\n",
    "                c = F.softmax(b_batch.view(-1, output_caps)).view(-1, input_caps, output_caps, 1)\n",
    "                s = (c * u_predict).sum(dim=1)\n",
    "                v = squash(s)\n",
    "        return v\n",
    "\n",
    "\n",
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, input_caps, input_dim, output_caps, output_dim, routing_module):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_caps = input_caps\n",
    "        self.output_dim = output_dim\n",
    "        self.output_caps = output_caps\n",
    "        self.weights = nn.Parameter(torch.Tensor(input_caps, input_dim, output_caps * output_dim))\n",
    "        self.routing_module = routing_module\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.input_caps)\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, caps_output):\n",
    "        caps_output = caps_output.unsqueeze(2)\n",
    "        u_predict = caps_output.matmul(self.weights)\n",
    "        u_predict = u_predict.view(u_predict.size(0), self.input_caps, self.output_caps, self.output_dim)\n",
    "        v = self.routing_module(u_predict)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruction Stuff \n",
    "class ReconstructionNet(nn.Module):\n",
    "    def __init__(self, n_dim=16, n_classes=10):\n",
    "        super(ReconstructionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_dim * n_classes, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 784)\n",
    "        self.n_dim = n_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        mask = Variable(torch.zeros((x.size()[0], self.n_classes)), requires_grad=False)\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask.scatter_(1, target.view(-1, 1), 1.)\n",
    "        mask = mask.unsqueeze(2)\n",
    "        x = x * mask\n",
    "        x = x.view(-1, self.n_dim * self.n_classes)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CapsNetWithReconstruction(nn.Module):\n",
    "    def __init__(self, capsnet, reconstruction_net):\n",
    "        super(CapsNetWithReconstruction, self).__init__()\n",
    "        self.capsnet = capsnet\n",
    "        self.reconstruction_net = reconstruction_net\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x, probs = self.capsnet(x)\n",
    "        reconstruction = self.reconstruction_net(x, target)\n",
    "        return reconstruction, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = [torch.DoubleTensor(d).view(28,28) for d in data]\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "TRAINDATASET = GeneralDataset(TRAINDATA,TRAINLABELS)\n",
    "VALDATASET = GeneralDataset(VALDATA,VALLABELS)\n",
    "TESTDATASET = GeneralDataset(TESTDATA,TESTLABELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Loss Functions\n",
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, m_pos, m_neg, lambda_):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, lengths, targets, size_average=True):\n",
    "        t = torch.zeros(lengths.size()).long()\n",
    "        if targets.is_cuda:\n",
    "            t = t.cuda()\n",
    "        t = t.scatter_(1, targets.data.view(-1, 1), 1)\n",
    "        targets = Variable(t)\n",
    "        losses = targets.float() * F.relu(self.m_pos - lengths).pow(2) + \\\n",
    "                 self.lambda_ * (1. - targets.float()) * F.relu(lengths - self.m_neg).pow(2)\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "### save model\n",
    "def saveModel(model,name,state_dict=False):\n",
    "    if state_dict:\n",
    "        torch.save(model.state_dict(),name)\n",
    "    else:\n",
    "        torch.save(model,name)\n",
    "\n",
    "\n",
    "### rotation transform class\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, degrees_tup):\n",
    "        self.dt = [0]*6\n",
    "        self.dt[degrees_tup] = 1\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if self.dt[0]:\n",
    "            return sample\n",
    "        sample = np.asarray(sample)\n",
    "       \n",
    "        if self.dt[0]:\n",
    "            sample = rotate(randint(0, 1),sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[1]:\n",
    "            sample = rotate(randint(0, 1)*2,sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[2]:\n",
    "            sample = rotate(randint(0, 1)*3,sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[3]:\n",
    "            sample = rotate(randint(1, 3),sample)\n",
    "            return np2PIL(sample)\n",
    "        if self.dt[4]:\n",
    "            sample = rotate(randint(0, 3),sample)\n",
    "            return np2PIL(sample)\n",
    "def np2PIL(data):\n",
    "    return Image.fromarray(data)\n",
    "def rotate(i,data):\n",
    "    if i == 0:\n",
    "        return data\n",
    "    elif i == 1:\n",
    "        return np.rot90(data)\n",
    "    elif i == 2:\n",
    "        return np.rot90(data,2)\n",
    "    elif i == 3:\n",
    "        return np.rot90(data,3)\n",
    "    \n",
    "### Normalize\n",
    "def normalize(tensorData):\n",
    "    return torch.Tensor(tensorData.numpy()/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Epoch,Loss,numStatsPerEpoch,Rec_loss_every=1):\n",
    "    model.train()\n",
    "    statIdx = [len(TRAINLOADER)/numStatsPerEpoch]*numStatsPerEpoch\n",
    "    statIdx = [(i)*a for i,a in enumerate(statIdx)]\n",
    "    for batch_idx, (data, target) in enumerate(TRAINLOADER):\n",
    "        \n",
    "        # get stats\n",
    "        if (batch_idx) in statIdx:\n",
    "            val_loss, val_acc,VN,VD = getLossAccND('Validation',VALLOADER)\n",
    "            test_loss, test_acc,TN,TD = getLossAccND('Test',TESTLOADER)\n",
    "            print\n",
    "            ValLoss.append(val_loss);ValAcc.append(val_acc);\n",
    "            TestLoss.append(test_loss);TestAcc.append(test_acc);\n",
    "    \n",
    "        # train\n",
    "        data = data.view(-1,1,28,28)\n",
    "        data, target = data.float(), target.long()\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        if RECONSTRUCTION:\n",
    "            output, probs = model(data, target)\n",
    "            reconstruction_loss = F.mse_loss(output, data.view(-1, 784))\n",
    "            margin_loss = loss_fn(probs, target)\n",
    "            loss = reconstruction_alpha * reconstruction_loss + margin_loss\n",
    "        else:\n",
    "            output, probs = model(data)\n",
    "            loss = loss_fn(probs, target)\n",
    "        \n",
    "        if batch_idx % Rec_loss_every == 0:\n",
    "            Loss.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INT == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                Epoch, batch_idx * len(data), len(TRAINLOADER.dataset),\n",
    "                       100. * batch_idx / len(TRAINLOADER), loss.data[0]))\n",
    "\n",
    "def getLossAccND(name,dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.view(-1,1,28,28)\n",
    "        data, target = data.float(), target.long()\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        #print data, target\n",
    "        if RECONSTRUCTION:\n",
    "            output, probs = model(data, target)\n",
    "            reconstruction_loss = F.mse_loss(output, data.view(-1, 784), size_average=False).data[0]\n",
    "            loss += loss_fn(probs, target, size_average=False).data[0]\n",
    "            loss += reconstruction_alpha * reconstruction_loss\n",
    "        else:\n",
    "            output, probs = model(data)\n",
    "            loss += loss_fn(probs, target, size_average=False).data[0]\n",
    "\n",
    "        pred = probs.data.max(1, keepdim=True)[1]  # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss /= len(dataloader.dataset)\n",
    "    print('{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        name,loss, correct, len(dataloader.dataset),\n",
    "        100. * correct / len(dataloader.dataset)))\n",
    "    return (loss, 100. * correct / len(dataloader.dataset), correct, len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining paramters, loaders, SGD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training settings\n",
    "# parser = argparse.ArgumentParser(description='CapsNet with MNIST')\n",
    "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "#                     help='input batch size for training (default: 64)')\n",
    "# parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "#                     help='input batch size for testing (default: 1000)')\n",
    "# parser.add_argument('--epochs', type=int, default=250, metavar='N',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                     help='learning rate (default: 0.01)')\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                     help='disables CUDA training')\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                     help='random seed (default: 1)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "# parser.add_argument('--routing_iterations', type=int, default=3)\n",
    "# parser.add_argument('--with_reconstruction', action='store_true', default=False)\n",
    "# args = parser.parse_args()\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "### Model Architecture Params\n",
    "ROUTING_ITERS = 3\n",
    "CONVKERNELSIZES, CONVKERNELSTRIDES, CONVNUMMAPS = \\\n",
    "    [7,7,5],[1,1,2],[256,256,512]\n",
    "PRECAPDEPTH = CONVNUMMAPS[-1]\n",
    "NUMCAPS = 32 \n",
    "NUMPERCAP = 16  #Note: precapdepth=numcaps*numpercap\n",
    "CAPFILTERSIZE = 6\n",
    "\n",
    "\n",
    "### Training Params\n",
    "BATCH_SIZE = 60\n",
    "TEST_BATCH_SIZE = 200\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_EPOCH = 3\n",
    "CUDA = torch.cuda.is_available()\n",
    "SEED = 1\n",
    "LOG_INT = 10 # how many batches to wait before logging training status\n",
    "RECONSTRUCTION = False\n",
    "ROTATION_TYPE = 3\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "\n",
    "### Stats\n",
    "NUMSTATSPEREPOCH = 4\n",
    "\n",
    "\n",
    "TRAINLOADER = DataLoader(TRAINDATASET, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, num_workers=4)\n",
    "VALLOADER = DataLoader(VALDATASET, batch_size=TEST_BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=4) \n",
    "TESTLOADER = DataLoader(TESTDATASET, batch_size=TEST_BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=4) \n",
    "\n",
    "model = CapsNet(ROUTING_ITERS,\n",
    "                PRECAPDEPTH, NUMCAPS, NUMPERCAP, CAPFILTERSIZE,\n",
    "                CONVKERNELSIZES,CONVKERNELSTRIDES,CONVNUMMAPS,\n",
    "                n_classes=10)\n",
    "#model = CapsNet( ROUTING_ITERS,PRECAPDEPTH, NUMCAPS, NUMPERCAP, CAPFILTERSIZE,\n",
    "#               CONVKERNELSIZES, CONVKERNELSTRIDES, CONVNUMMAPS )\n",
    "\n",
    "if RECONSTRUCTION:\n",
    "    reconstruction_model = ReconstructionNet(16, 10)\n",
    "    reconstruction_alpha = 0.0005\n",
    "    model = CapsNetWithReconstruction(model, reconstruction_model)\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "loss_fn = MarginLoss(0.9, 0.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.8096, Accuracy: 702/6000 (12%)\n",
      "Test set: Average loss: 0.8096, Accuracy: 1168/10000 (12%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.080962\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 0.057008\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.056410\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 0.054109\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.048548\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.033196\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.024748\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 0.016236\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.020149\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 0.010982\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.011152\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 0.011117\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.007979\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 0.006620\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 0.008496\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.008717\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.004739\n",
      "Train Epoch: 1 [10200/60000 (17%)]\tLoss: 0.005579\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 0.004347\n",
      "Train Epoch: 1 [11400/60000 (19%)]\tLoss: 0.007629\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.004930\n",
      "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 0.005152\n",
      "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 0.007318\n",
      "Train Epoch: 1 [13800/60000 (23%)]\tLoss: 0.002921\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.002930\n",
      "Validation set: Average loss: 0.0389, Accuracy: 5833/6000 (97%)\n",
      "Test set: Average loss: 0.0368, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.003507\n",
      "Train Epoch: 1 [15600/60000 (26%)]\tLoss: 0.002145\n",
      "Train Epoch: 1 [16200/60000 (27%)]\tLoss: 0.002627\n",
      "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 0.004518\n",
      "Train Epoch: 1 [17400/60000 (29%)]\tLoss: 0.003366\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.002471\n",
      "Train Epoch: 1 [18600/60000 (31%)]\tLoss: 0.002776\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.002991\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tLoss: 0.003872\n",
      "Train Epoch: 1 [20400/60000 (34%)]\tLoss: 0.002241\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.002810\n",
      "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 0.003030\n",
      "Train Epoch: 1 [22200/60000 (37%)]\tLoss: 0.003766\n",
      "Train Epoch: 1 [22800/60000 (38%)]\tLoss: 0.002775\n",
      "Train Epoch: 1 [23400/60000 (39%)]\tLoss: 0.002389\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.003666\n",
      "Train Epoch: 1 [24600/60000 (41%)]\tLoss: 0.001765\n",
      "Train Epoch: 1 [25200/60000 (42%)]\tLoss: 0.002689\n",
      "Train Epoch: 1 [25800/60000 (43%)]\tLoss: 0.002640\n",
      "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 0.001596\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.001700\n",
      "Train Epoch: 1 [27600/60000 (46%)]\tLoss: 0.003181\n",
      "Train Epoch: 1 [28200/60000 (47%)]\tLoss: 0.003619\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.001254\n",
      "Train Epoch: 1 [29400/60000 (49%)]\tLoss: 0.001863\n",
      "Validation set: Average loss: 0.0206, Accuracy: 5917/6000 (99%)\n",
      "Test set: Average loss: 0.0206, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.001826\n",
      "Train Epoch: 1 [30600/60000 (51%)]\tLoss: 0.001413\n",
      "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 0.001287\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tLoss: 0.002399\n",
      "Train Epoch: 1 [32400/60000 (54%)]\tLoss: 0.001395\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.002975\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.003730\n",
      "Train Epoch: 1 [34200/60000 (57%)]\tLoss: 0.001721\n",
      "Train Epoch: 1 [34800/60000 (58%)]\tLoss: 0.002516\n",
      "Train Epoch: 1 [35400/60000 (59%)]\tLoss: 0.001464\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.001827\n",
      "Train Epoch: 1 [36600/60000 (61%)]\tLoss: 0.001144\n",
      "Train Epoch: 1 [37200/60000 (62%)]\tLoss: 0.001972\n",
      "Train Epoch: 1 [37800/60000 (63%)]\tLoss: 0.002086\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.003473\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.001584\n",
      "Train Epoch: 1 [39600/60000 (66%)]\tLoss: 0.000994\n",
      "Train Epoch: 1 [40200/60000 (67%)]\tLoss: 0.000857\n",
      "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 0.001930\n",
      "Train Epoch: 1 [41400/60000 (69%)]\tLoss: 0.002076\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.002227\n",
      "Train Epoch: 1 [42600/60000 (71%)]\tLoss: 0.002218\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.000709\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tLoss: 0.002989\n",
      "Train Epoch: 1 [44400/60000 (74%)]\tLoss: 0.001044\n",
      "Validation set: Average loss: 0.0175, Accuracy: 5931/6000 (99%)\n",
      "Test set: Average loss: 0.0172, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.001073\n",
      "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 0.001771\n",
      "Train Epoch: 1 [46200/60000 (77%)]\tLoss: 0.002851\n",
      "Train Epoch: 1 [46800/60000 (78%)]\tLoss: 0.001042\n",
      "Train Epoch: 1 [47400/60000 (79%)]\tLoss: 0.001109\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.001682\n",
      "Train Epoch: 1 [48600/60000 (81%)]\tLoss: 0.000990\n",
      "Train Epoch: 1 [49200/60000 (82%)]\tLoss: 0.001822\n",
      "Train Epoch: 1 [49800/60000 (83%)]\tLoss: 0.003479\n",
      "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 0.003910\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.001630\n",
      "Train Epoch: 1 [51600/60000 (86%)]\tLoss: 0.001240\n",
      "Train Epoch: 1 [52200/60000 (87%)]\tLoss: 0.001859\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.002181\n",
      "Train Epoch: 1 [53400/60000 (89%)]\tLoss: 0.001211\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.000325\n",
      "Train Epoch: 1 [54600/60000 (91%)]\tLoss: 0.000529\n",
      "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 0.002772\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tLoss: 0.002346\n",
      "Train Epoch: 1 [56400/60000 (94%)]\tLoss: 0.003284\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.001865\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001091\n",
      "Train Epoch: 1 [58200/60000 (97%)]\tLoss: 0.002255\n",
      "Train Epoch: 1 [58800/60000 (98%)]\tLoss: 0.001083\n",
      "Train Epoch: 1 [59400/60000 (99%)]\tLoss: 0.001100\n",
      "Validation set: Average loss: 0.0132, Accuracy: 5934/6000 (99%)\n",
      "Test set: Average loss: 0.0137, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.000409\n",
      "Train Epoch: 2 [600/60000 (1%)]\tLoss: 0.000712\n",
      "Train Epoch: 2 [1200/60000 (2%)]\tLoss: 0.001534\n",
      "Train Epoch: 2 [1800/60000 (3%)]\tLoss: 0.000323\n",
      "Train Epoch: 2 [2400/60000 (4%)]\tLoss: 0.000863\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.001166\n",
      "Train Epoch: 2 [3600/60000 (6%)]\tLoss: 0.001298\n",
      "Train Epoch: 2 [4200/60000 (7%)]\tLoss: 0.000330\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.000792\n",
      "Train Epoch: 2 [5400/60000 (9%)]\tLoss: 0.001935\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.002347\n",
      "Train Epoch: 2 [6600/60000 (11%)]\tLoss: 0.000717\n",
      "Train Epoch: 2 [7200/60000 (12%)]\tLoss: 0.001000\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tLoss: 0.001746\n",
      "Train Epoch: 2 [8400/60000 (14%)]\tLoss: 0.001263\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.000505\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.000453\n",
      "Train Epoch: 2 [10200/60000 (17%)]\tLoss: 0.003120\n",
      "Train Epoch: 2 [10800/60000 (18%)]\tLoss: 0.000223\n",
      "Train Epoch: 2 [11400/60000 (19%)]\tLoss: 0.001107\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.001979\n",
      "Train Epoch: 2 [12600/60000 (21%)]\tLoss: 0.000871\n",
      "Train Epoch: 2 [13200/60000 (22%)]\tLoss: 0.001413\n",
      "Train Epoch: 2 [13800/60000 (23%)]\tLoss: 0.000752\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.000853\n",
      "Validation set: Average loss: 0.0120, Accuracy: 5951/6000 (99%)\n",
      "Test set: Average loss: 0.0134, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.000838\n",
      "Train Epoch: 2 [15600/60000 (26%)]\tLoss: 0.001035\n",
      "Train Epoch: 2 [16200/60000 (27%)]\tLoss: 0.003324\n",
      "Train Epoch: 2 [16800/60000 (28%)]\tLoss: 0.000744\n",
      "Train Epoch: 2 [17400/60000 (29%)]\tLoss: 0.001341\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.000421\n",
      "Train Epoch: 2 [18600/60000 (31%)]\tLoss: 0.000224\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.000576\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tLoss: 0.001266\n",
      "Train Epoch: 2 [20400/60000 (34%)]\tLoss: 0.002167\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.000827\n",
      "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 0.001650\n",
      "Train Epoch: 2 [22200/60000 (37%)]\tLoss: 0.002256\n",
      "Train Epoch: 2 [22800/60000 (38%)]\tLoss: 0.000683\n",
      "Train Epoch: 2 [23400/60000 (39%)]\tLoss: 0.001507\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.000908\n",
      "Train Epoch: 2 [24600/60000 (41%)]\tLoss: 0.000563\n",
      "Train Epoch: 2 [25200/60000 (42%)]\tLoss: 0.000774\n",
      "Train Epoch: 2 [25800/60000 (43%)]\tLoss: 0.001633\n",
      "Train Epoch: 2 [26400/60000 (44%)]\tLoss: 0.000599\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.000432\n",
      "Train Epoch: 2 [27600/60000 (46%)]\tLoss: 0.000923\n",
      "Train Epoch: 2 [28200/60000 (47%)]\tLoss: 0.000858\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.001044\n",
      "Train Epoch: 2 [29400/60000 (49%)]\tLoss: 0.000877\n",
      "Validation set: Average loss: 0.0106, Accuracy: 5947/6000 (99%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0117, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.000653\n",
      "Train Epoch: 2 [30600/60000 (51%)]\tLoss: 0.000893\n",
      "Train Epoch: 2 [31200/60000 (52%)]\tLoss: 0.000352\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tLoss: 0.002261\n",
      "Train Epoch: 2 [32400/60000 (54%)]\tLoss: 0.000799\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.001610\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.000692\n",
      "Train Epoch: 2 [34200/60000 (57%)]\tLoss: 0.001477\n",
      "Train Epoch: 2 [34800/60000 (58%)]\tLoss: 0.000845\n",
      "Train Epoch: 2 [35400/60000 (59%)]\tLoss: 0.002683\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.000893\n",
      "Train Epoch: 2 [36600/60000 (61%)]\tLoss: 0.000524\n",
      "Train Epoch: 2 [37200/60000 (62%)]\tLoss: 0.000723\n",
      "Train Epoch: 2 [37800/60000 (63%)]\tLoss: 0.000435\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.003328\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.001142\n",
      "Train Epoch: 2 [39600/60000 (66%)]\tLoss: 0.001657\n",
      "Train Epoch: 2 [40200/60000 (67%)]\tLoss: 0.001236\n",
      "Train Epoch: 2 [40800/60000 (68%)]\tLoss: 0.001139\n",
      "Train Epoch: 2 [41400/60000 (69%)]\tLoss: 0.001016\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.000877\n",
      "Train Epoch: 2 [42600/60000 (71%)]\tLoss: 0.000180\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.001059\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tLoss: 0.002754\n",
      "Train Epoch: 2 [44400/60000 (74%)]\tLoss: 0.001441\n",
      "Validation set: Average loss: 0.0127, Accuracy: 5949/6000 (99%)\n",
      "Test set: Average loss: 0.0149, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.000863\n",
      "Train Epoch: 2 [45600/60000 (76%)]\tLoss: 0.000821\n",
      "Train Epoch: 2 [46200/60000 (77%)]\tLoss: 0.000467\n",
      "Train Epoch: 2 [46800/60000 (78%)]\tLoss: 0.000773\n",
      "Train Epoch: 2 [47400/60000 (79%)]\tLoss: 0.000418\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.000776\n",
      "Train Epoch: 2 [48600/60000 (81%)]\tLoss: 0.002048\n",
      "Train Epoch: 2 [49200/60000 (82%)]\tLoss: 0.000885\n",
      "Train Epoch: 2 [49800/60000 (83%)]\tLoss: 0.001733\n",
      "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 0.000642\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.001416\n",
      "Train Epoch: 2 [51600/60000 (86%)]\tLoss: 0.001056\n",
      "Train Epoch: 2 [52200/60000 (87%)]\tLoss: 0.000882\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.000671\n",
      "Train Epoch: 2 [53400/60000 (89%)]\tLoss: 0.000990\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.000779\n",
      "Train Epoch: 2 [54600/60000 (91%)]\tLoss: 0.000534\n",
      "Train Epoch: 2 [55200/60000 (92%)]\tLoss: 0.002117\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tLoss: 0.001882\n",
      "Train Epoch: 2 [56400/60000 (94%)]\tLoss: 0.001178\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.000681\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.000948\n",
      "Train Epoch: 2 [58200/60000 (97%)]\tLoss: 0.001421\n",
      "Train Epoch: 2 [58800/60000 (98%)]\tLoss: 0.001507\n",
      "Train Epoch: 2 [59400/60000 (99%)]\tLoss: 0.001409\n",
      "Validation set: Average loss: 0.0106, Accuracy: 5948/6000 (99%)\n",
      "Test set: Average loss: 0.0122, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001175\n",
      "Train Epoch: 3 [600/60000 (1%)]\tLoss: 0.000298\n",
      "Train Epoch: 3 [1200/60000 (2%)]\tLoss: 0.000673\n",
      "Train Epoch: 3 [1800/60000 (3%)]\tLoss: 0.001920\n",
      "Train Epoch: 3 [2400/60000 (4%)]\tLoss: 0.001160\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.002256\n",
      "Train Epoch: 3 [3600/60000 (6%)]\tLoss: 0.000869\n",
      "Train Epoch: 3 [4200/60000 (7%)]\tLoss: 0.000470\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.000250\n",
      "Train Epoch: 3 [5400/60000 (9%)]\tLoss: 0.000693\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.000531\n",
      "Train Epoch: 3 [6600/60000 (11%)]\tLoss: 0.001075\n",
      "Train Epoch: 3 [7200/60000 (12%)]\tLoss: 0.000686\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tLoss: 0.000490\n",
      "Train Epoch: 3 [8400/60000 (14%)]\tLoss: 0.001342\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.000511\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.000476\n",
      "Train Epoch: 3 [10200/60000 (17%)]\tLoss: 0.000611\n",
      "Train Epoch: 3 [10800/60000 (18%)]\tLoss: 0.000290\n",
      "Train Epoch: 3 [11400/60000 (19%)]\tLoss: 0.000487\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.001868\n",
      "Train Epoch: 3 [12600/60000 (21%)]\tLoss: 0.000829\n",
      "Train Epoch: 3 [13200/60000 (22%)]\tLoss: 0.000413\n",
      "Train Epoch: 3 [13800/60000 (23%)]\tLoss: 0.000658\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.001536\n",
      "Validation set: Average loss: 0.0112, Accuracy: 5961/6000 (99%)\n",
      "Test set: Average loss: 0.0133, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.000828\n",
      "Train Epoch: 3 [15600/60000 (26%)]\tLoss: 0.001110\n",
      "Train Epoch: 3 [16200/60000 (27%)]\tLoss: 0.000488\n",
      "Train Epoch: 3 [16800/60000 (28%)]\tLoss: 0.000167\n",
      "Train Epoch: 3 [17400/60000 (29%)]\tLoss: 0.000474\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.000134\n",
      "Train Epoch: 3 [18600/60000 (31%)]\tLoss: 0.000544\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001428\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tLoss: 0.000094\n",
      "Train Epoch: 3 [20400/60000 (34%)]\tLoss: 0.001087\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.000470\n",
      "Train Epoch: 3 [21600/60000 (36%)]\tLoss: 0.001044\n",
      "Train Epoch: 3 [22200/60000 (37%)]\tLoss: 0.000326\n",
      "Train Epoch: 3 [22800/60000 (38%)]\tLoss: 0.001081\n",
      "Train Epoch: 3 [23400/60000 (39%)]\tLoss: 0.000604\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.002159\n",
      "Train Epoch: 3 [24600/60000 (41%)]\tLoss: 0.002910\n",
      "Train Epoch: 3 [25200/60000 (42%)]\tLoss: 0.001042\n",
      "Train Epoch: 3 [25800/60000 (43%)]\tLoss: 0.000840\n",
      "Train Epoch: 3 [26400/60000 (44%)]\tLoss: 0.000426\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.001357\n",
      "Train Epoch: 3 [27600/60000 (46%)]\tLoss: 0.001135\n",
      "Train Epoch: 3 [28200/60000 (47%)]\tLoss: 0.001492\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.001792\n",
      "Train Epoch: 3 [29400/60000 (49%)]\tLoss: 0.001246\n",
      "Validation set: Average loss: 0.0084, Accuracy: 5962/6000 (99%)\n",
      "Test set: Average loss: 0.0108, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.001021\n",
      "Train Epoch: 3 [30600/60000 (51%)]\tLoss: 0.000181\n",
      "Train Epoch: 3 [31200/60000 (52%)]\tLoss: 0.000960\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tLoss: 0.000424\n",
      "Train Epoch: 3 [32400/60000 (54%)]\tLoss: 0.000968\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.001656\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.000295\n",
      "Train Epoch: 3 [34200/60000 (57%)]\tLoss: 0.000983\n",
      "Train Epoch: 3 [34800/60000 (58%)]\tLoss: 0.000609\n",
      "Train Epoch: 3 [35400/60000 (59%)]\tLoss: 0.001352\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.001083\n",
      "Train Epoch: 3 [36600/60000 (61%)]\tLoss: 0.001264\n",
      "Train Epoch: 3 [37200/60000 (62%)]\tLoss: 0.000089\n",
      "Train Epoch: 3 [37800/60000 (63%)]\tLoss: 0.000122\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.000272\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.001639\n",
      "Train Epoch: 3 [39600/60000 (66%)]\tLoss: 0.000203\n",
      "Train Epoch: 3 [40200/60000 (67%)]\tLoss: 0.000214\n",
      "Train Epoch: 3 [40800/60000 (68%)]\tLoss: 0.001408\n",
      "Train Epoch: 3 [41400/60000 (69%)]\tLoss: 0.001883\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.002020\n",
      "Train Epoch: 3 [42600/60000 (71%)]\tLoss: 0.000272\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.000283\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tLoss: 0.000343\n",
      "Train Epoch: 3 [44400/60000 (74%)]\tLoss: 0.000220\n",
      "Validation set: Average loss: 0.0067, Accuracy: 5972/6000 (100%)\n",
      "Test set: Average loss: 0.0096, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.000708\n",
      "Train Epoch: 3 [45600/60000 (76%)]\tLoss: 0.000630\n",
      "Train Epoch: 3 [46200/60000 (77%)]\tLoss: 0.000210\n",
      "Train Epoch: 3 [46800/60000 (78%)]\tLoss: 0.001522\n",
      "Train Epoch: 3 [47400/60000 (79%)]\tLoss: 0.001229\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.000758\n",
      "Train Epoch: 3 [48600/60000 (81%)]\tLoss: 0.000416\n",
      "Train Epoch: 3 [49200/60000 (82%)]\tLoss: 0.001313\n",
      "Train Epoch: 3 [49800/60000 (83%)]\tLoss: 0.000548\n",
      "Train Epoch: 3 [50400/60000 (84%)]\tLoss: 0.000137\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.000695\n",
      "Train Epoch: 3 [51600/60000 (86%)]\tLoss: 0.000616\n",
      "Train Epoch: 3 [52200/60000 (87%)]\tLoss: 0.001355\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.001601\n",
      "Train Epoch: 3 [53400/60000 (89%)]\tLoss: 0.000256\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.000171\n",
      "Train Epoch: 3 [54600/60000 (91%)]\tLoss: 0.002296\n",
      "Train Epoch: 3 [55200/60000 (92%)]\tLoss: 0.000360\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tLoss: 0.000437\n",
      "Train Epoch: 3 [56400/60000 (94%)]\tLoss: 0.000165\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.000333\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.000971\n",
      "Train Epoch: 3 [58200/60000 (97%)]\tLoss: 0.002074\n",
      "Train Epoch: 3 [58800/60000 (98%)]\tLoss: 0.000959\n",
      "Train Epoch: 3 [59400/60000 (99%)]\tLoss: 0.001200\n",
      "Validation set: Average loss: 0.0083, Accuracy: 5965/6000 (99%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0116, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001227\n",
      "Train Epoch: 4 [600/60000 (1%)]\tLoss: 0.002220\n",
      "Train Epoch: 4 [1200/60000 (2%)]\tLoss: 0.000288\n",
      "Train Epoch: 4 [1800/60000 (3%)]\tLoss: 0.000396\n",
      "Train Epoch: 4 [2400/60000 (4%)]\tLoss: 0.000568\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.000180\n",
      "Train Epoch: 4 [3600/60000 (6%)]\tLoss: 0.000375\n",
      "Train Epoch: 4 [4200/60000 (7%)]\tLoss: 0.000385\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.000176\n",
      "Train Epoch: 4 [5400/60000 (9%)]\tLoss: 0.000526\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.000618\n",
      "Train Epoch: 4 [6600/60000 (11%)]\tLoss: 0.000474\n",
      "Train Epoch: 4 [7200/60000 (12%)]\tLoss: 0.000078\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tLoss: 0.000180\n",
      "Train Epoch: 4 [8400/60000 (14%)]\tLoss: 0.001394\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.000837\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.000187\n",
      "Train Epoch: 4 [10200/60000 (17%)]\tLoss: 0.000730\n",
      "Train Epoch: 4 [10800/60000 (18%)]\tLoss: 0.001244\n",
      "Train Epoch: 4 [11400/60000 (19%)]\tLoss: 0.000410\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.000345\n",
      "Train Epoch: 4 [12600/60000 (21%)]\tLoss: 0.000391\n",
      "Train Epoch: 4 [13200/60000 (22%)]\tLoss: 0.000138\n",
      "Train Epoch: 4 [13800/60000 (23%)]\tLoss: 0.001170\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.000443\n",
      "Validation set: Average loss: 0.0050, Accuracy: 5980/6000 (100%)\n",
      "Test set: Average loss: 0.0083, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.000131\n",
      "Train Epoch: 4 [15600/60000 (26%)]\tLoss: 0.001547\n",
      "Train Epoch: 4 [16200/60000 (27%)]\tLoss: 0.000537\n",
      "Train Epoch: 4 [16800/60000 (28%)]\tLoss: 0.000297\n",
      "Train Epoch: 4 [17400/60000 (29%)]\tLoss: 0.000115\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.000271\n",
      "Train Epoch: 4 [18600/60000 (31%)]\tLoss: 0.001340\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001795\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tLoss: 0.000484\n",
      "Train Epoch: 4 [20400/60000 (34%)]\tLoss: 0.000283\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.000133\n",
      "Train Epoch: 4 [21600/60000 (36%)]\tLoss: 0.000164\n",
      "Train Epoch: 4 [22200/60000 (37%)]\tLoss: 0.000438\n",
      "Train Epoch: 4 [22800/60000 (38%)]\tLoss: 0.000629\n",
      "Train Epoch: 4 [23400/60000 (39%)]\tLoss: 0.001229\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.000479\n",
      "Train Epoch: 4 [24600/60000 (41%)]\tLoss: 0.000130\n",
      "Train Epoch: 4 [25200/60000 (42%)]\tLoss: 0.001066\n",
      "Train Epoch: 4 [25800/60000 (43%)]\tLoss: 0.000387\n",
      "Train Epoch: 4 [26400/60000 (44%)]\tLoss: 0.000640\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.000384\n",
      "Train Epoch: 4 [27600/60000 (46%)]\tLoss: 0.000657\n",
      "Train Epoch: 4 [28200/60000 (47%)]\tLoss: 0.000533\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.000929\n",
      "Train Epoch: 4 [29400/60000 (49%)]\tLoss: 0.000814\n",
      "Validation set: Average loss: 0.0084, Accuracy: 5977/6000 (100%)\n",
      "Test set: Average loss: 0.0120, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.000367\n",
      "Train Epoch: 4 [30600/60000 (51%)]\tLoss: 0.000438\n",
      "Train Epoch: 4 [31200/60000 (52%)]\tLoss: 0.000589\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tLoss: 0.000304\n",
      "Train Epoch: 4 [32400/60000 (54%)]\tLoss: 0.000085\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.001881\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.000353\n",
      "Train Epoch: 4 [34200/60000 (57%)]\tLoss: 0.000197\n",
      "Train Epoch: 4 [34800/60000 (58%)]\tLoss: 0.000811\n",
      "Train Epoch: 4 [35400/60000 (59%)]\tLoss: 0.001442\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.000365\n",
      "Train Epoch: 4 [36600/60000 (61%)]\tLoss: 0.000865\n",
      "Train Epoch: 4 [37200/60000 (62%)]\tLoss: 0.000561\n",
      "Train Epoch: 4 [37800/60000 (63%)]\tLoss: 0.000290\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000767\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.002463\n",
      "Train Epoch: 4 [39600/60000 (66%)]\tLoss: 0.000307\n",
      "Train Epoch: 4 [40200/60000 (67%)]\tLoss: 0.000803\n",
      "Train Epoch: 4 [40800/60000 (68%)]\tLoss: 0.000351\n",
      "Train Epoch: 4 [41400/60000 (69%)]\tLoss: 0.000207\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.000731\n",
      "Train Epoch: 4 [42600/60000 (71%)]\tLoss: 0.000386\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.001659\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tLoss: 0.000731\n",
      "Train Epoch: 4 [44400/60000 (74%)]\tLoss: 0.001074\n",
      "Validation set: Average loss: 0.0055, Accuracy: 5974/6000 (100%)\n",
      "Test set: Average loss: 0.0084, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.000536\n",
      "Train Epoch: 4 [45600/60000 (76%)]\tLoss: 0.000749\n",
      "Train Epoch: 4 [46200/60000 (77%)]\tLoss: 0.000272\n",
      "Train Epoch: 4 [46800/60000 (78%)]\tLoss: 0.000555\n",
      "Train Epoch: 4 [47400/60000 (79%)]\tLoss: 0.000242\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.000587\n",
      "Train Epoch: 4 [48600/60000 (81%)]\tLoss: 0.001549\n",
      "Train Epoch: 4 [49200/60000 (82%)]\tLoss: 0.000345\n",
      "Train Epoch: 4 [49800/60000 (83%)]\tLoss: 0.001929\n",
      "Train Epoch: 4 [50400/60000 (84%)]\tLoss: 0.000824\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.001793\n",
      "Train Epoch: 4 [51600/60000 (86%)]\tLoss: 0.000628\n",
      "Train Epoch: 4 [52200/60000 (87%)]\tLoss: 0.000566\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.000352\n",
      "Train Epoch: 4 [53400/60000 (89%)]\tLoss: 0.003115\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.000684\n",
      "Train Epoch: 4 [54600/60000 (91%)]\tLoss: 0.000529\n",
      "Train Epoch: 4 [55200/60000 (92%)]\tLoss: 0.000139\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tLoss: 0.000472\n",
      "Train Epoch: 4 [56400/60000 (94%)]\tLoss: 0.000411\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.000286\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.000025\n",
      "Train Epoch: 4 [58200/60000 (97%)]\tLoss: 0.000251\n",
      "Train Epoch: 4 [58800/60000 (98%)]\tLoss: 0.000919\n",
      "Train Epoch: 4 [59400/60000 (99%)]\tLoss: 0.000067\n",
      "Epoch     3: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation set: Average loss: 0.0058, Accuracy: 5979/6000 (100%)\n",
      "Test set: Average loss: 0.0096, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000342\n",
      "Train Epoch: 5 [600/60000 (1%)]\tLoss: 0.000112\n",
      "Train Epoch: 5 [1200/60000 (2%)]\tLoss: 0.000025\n",
      "Train Epoch: 5 [1800/60000 (3%)]\tLoss: 0.000010\n",
      "Train Epoch: 5 [2400/60000 (4%)]\tLoss: 0.000166\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.000781\n",
      "Train Epoch: 5 [3600/60000 (6%)]\tLoss: 0.000043\n",
      "Train Epoch: 5 [4200/60000 (7%)]\tLoss: 0.000264\n",
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.000835\n",
      "Train Epoch: 5 [5400/60000 (9%)]\tLoss: 0.000018\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.000026\n",
      "Train Epoch: 5 [6600/60000 (11%)]\tLoss: 0.000070\n",
      "Train Epoch: 5 [7200/60000 (12%)]\tLoss: 0.000092\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tLoss: 0.000268\n",
      "Train Epoch: 5 [8400/60000 (14%)]\tLoss: 0.000135\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.000431\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.000115\n",
      "Train Epoch: 5 [10200/60000 (17%)]\tLoss: 0.000063\n",
      "Train Epoch: 5 [10800/60000 (18%)]\tLoss: 0.000049\n",
      "Train Epoch: 5 [11400/60000 (19%)]\tLoss: 0.000040\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.000073\n",
      "Train Epoch: 5 [12600/60000 (21%)]\tLoss: 0.000060\n",
      "Train Epoch: 5 [13200/60000 (22%)]\tLoss: 0.001199\n",
      "Train Epoch: 5 [13800/60000 (23%)]\tLoss: 0.000016\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.000031\n",
      "Validation set: Average loss: 0.0022, Accuracy: 5990/6000 (100%)\n",
      "Test set: Average loss: 0.0055, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.000006\n",
      "Train Epoch: 5 [15600/60000 (26%)]\tLoss: 0.000631\n",
      "Train Epoch: 5 [16200/60000 (27%)]\tLoss: 0.000032\n",
      "Train Epoch: 5 [16800/60000 (28%)]\tLoss: 0.000050\n",
      "Train Epoch: 5 [17400/60000 (29%)]\tLoss: 0.000228\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.000193\n",
      "Train Epoch: 5 [18600/60000 (31%)]\tLoss: 0.000363\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000046\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tLoss: 0.001595\n",
      "Train Epoch: 5 [20400/60000 (34%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.000014\n",
      "Train Epoch: 5 [21600/60000 (36%)]\tLoss: 0.000677\n",
      "Train Epoch: 5 [22200/60000 (37%)]\tLoss: 0.000025\n",
      "Train Epoch: 5 [22800/60000 (38%)]\tLoss: 0.000126\n",
      "Train Epoch: 5 [23400/60000 (39%)]\tLoss: 0.000044\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.000500\n",
      "Train Epoch: 5 [24600/60000 (41%)]\tLoss: 0.000128\n",
      "Train Epoch: 5 [25200/60000 (42%)]\tLoss: 0.000099\n",
      "Train Epoch: 5 [25800/60000 (43%)]\tLoss: 0.000077\n",
      "Train Epoch: 5 [26400/60000 (44%)]\tLoss: 0.000074\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.000128\n",
      "Train Epoch: 5 [27600/60000 (46%)]\tLoss: 0.000117\n",
      "Train Epoch: 5 [28200/60000 (47%)]\tLoss: 0.000910\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.000049\n",
      "Train Epoch: 5 [29400/60000 (49%)]\tLoss: 0.000032\n",
      "Validation set: Average loss: 0.0018, Accuracy: 5991/6000 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0052, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.000211\n",
      "Train Epoch: 5 [30600/60000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [31200/60000 (52%)]\tLoss: 0.000012\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tLoss: 0.000105\n",
      "Train Epoch: 5 [32400/60000 (54%)]\tLoss: 0.000040\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.000228\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.000026\n",
      "Train Epoch: 5 [34200/60000 (57%)]\tLoss: 0.000025\n",
      "Train Epoch: 5 [34800/60000 (58%)]\tLoss: 0.000272\n",
      "Train Epoch: 5 [35400/60000 (59%)]\tLoss: 0.000123\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.000072\n",
      "Train Epoch: 5 [36600/60000 (61%)]\tLoss: 0.000046\n",
      "Train Epoch: 5 [37200/60000 (62%)]\tLoss: 0.000023\n",
      "Train Epoch: 5 [37800/60000 (63%)]\tLoss: 0.001126\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000584\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.000007\n",
      "Train Epoch: 5 [39600/60000 (66%)]\tLoss: 0.000006\n",
      "Train Epoch: 5 [40200/60000 (67%)]\tLoss: 0.000044\n",
      "Train Epoch: 5 [40800/60000 (68%)]\tLoss: 0.000063\n",
      "Train Epoch: 5 [41400/60000 (69%)]\tLoss: 0.000047\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.000028\n",
      "Train Epoch: 5 [42600/60000 (71%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.000007\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tLoss: 0.000034\n",
      "Train Epoch: 5 [44400/60000 (74%)]\tLoss: 0.000047\n",
      "Validation set: Average loss: 0.0016, Accuracy: 5992/6000 (100%)\n",
      "Test set: Average loss: 0.0052, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.000044\n",
      "Train Epoch: 5 [45600/60000 (76%)]\tLoss: 0.000262\n",
      "Train Epoch: 5 [46200/60000 (77%)]\tLoss: 0.000005\n",
      "Train Epoch: 5 [46800/60000 (78%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [47400/60000 (79%)]\tLoss: 0.000129\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.000021\n",
      "Train Epoch: 5 [48600/60000 (81%)]\tLoss: 0.000510\n",
      "Train Epoch: 5 [49200/60000 (82%)]\tLoss: 0.000041\n",
      "Train Epoch: 5 [49800/60000 (83%)]\tLoss: 0.000019\n",
      "Train Epoch: 5 [50400/60000 (84%)]\tLoss: 0.000294\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.000027\n",
      "Train Epoch: 5 [51600/60000 (86%)]\tLoss: 0.000055\n",
      "Train Epoch: 5 [52200/60000 (87%)]\tLoss: 0.000018\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.000043\n",
      "Train Epoch: 5 [53400/60000 (89%)]\tLoss: 0.000270\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.000637\n",
      "Train Epoch: 5 [54600/60000 (91%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [55200/60000 (92%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tLoss: 0.000122\n",
      "Train Epoch: 5 [56400/60000 (94%)]\tLoss: 0.000141\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.000114\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000117\n",
      "Train Epoch: 5 [58200/60000 (97%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [58800/60000 (98%)]\tLoss: 0.000333\n",
      "Train Epoch: 5 [59400/60000 (99%)]\tLoss: 0.000003\n",
      "Validation set: Average loss: 0.0015, Accuracy: 5991/6000 (100%)\n",
      "Test set: Average loss: 0.0049, Accuracy: 9952/10000 (100%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.000065\n",
      "Train Epoch: 6 [600/60000 (1%)]\tLoss: 0.000180\n",
      "Train Epoch: 6 [1200/60000 (2%)]\tLoss: 0.000013\n",
      "Train Epoch: 6 [1800/60000 (3%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [2400/60000 (4%)]\tLoss: 0.000026\n",
      "Train Epoch: 6 [3000/60000 (5%)]\tLoss: 0.000004\n",
      "Train Epoch: 6 [3600/60000 (6%)]\tLoss: 0.000021\n",
      "Train Epoch: 6 [4200/60000 (7%)]\tLoss: 0.000048\n",
      "Train Epoch: 6 [4800/60000 (8%)]\tLoss: 0.000145\n",
      "Train Epoch: 6 [5400/60000 (9%)]\tLoss: 0.000022\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.000029\n",
      "Train Epoch: 6 [6600/60000 (11%)]\tLoss: 0.000011\n",
      "Train Epoch: 6 [7200/60000 (12%)]\tLoss: 0.000010\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tLoss: 0.000135\n",
      "Train Epoch: 6 [8400/60000 (14%)]\tLoss: 0.000041\n",
      "Train Epoch: 6 [9000/60000 (15%)]\tLoss: 0.000023\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.000274\n",
      "Train Epoch: 6 [10200/60000 (17%)]\tLoss: 0.000030\n",
      "Train Epoch: 6 [10800/60000 (18%)]\tLoss: 0.000084\n",
      "Train Epoch: 6 [11400/60000 (19%)]\tLoss: 0.000654\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.000115\n",
      "Train Epoch: 6 [12600/60000 (21%)]\tLoss: 0.000008\n",
      "Train Epoch: 6 [13200/60000 (22%)]\tLoss: 0.000613\n",
      "Train Epoch: 6 [13800/60000 (23%)]\tLoss: 0.000010\n",
      "Train Epoch: 6 [14400/60000 (24%)]\tLoss: 0.000179\n",
      "Validation set: Average loss: 0.0014, Accuracy: 5992/6000 (100%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 0.000031\n",
      "Train Epoch: 6 [15600/60000 (26%)]\tLoss: 0.000029\n",
      "Train Epoch: 6 [16200/60000 (27%)]\tLoss: 0.000138\n",
      "Train Epoch: 6 [16800/60000 (28%)]\tLoss: 0.000011\n",
      "Train Epoch: 6 [17400/60000 (29%)]\tLoss: 0.000171\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.000029\n",
      "Train Epoch: 6 [18600/60000 (31%)]\tLoss: 0.000007\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.000038\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tLoss: 0.001816\n",
      "Train Epoch: 6 [20400/60000 (34%)]\tLoss: 0.000156\n",
      "Train Epoch: 6 [21000/60000 (35%)]\tLoss: 0.000149\n",
      "Train Epoch: 6 [21600/60000 (36%)]\tLoss: 0.000012\n",
      "Train Epoch: 6 [22200/60000 (37%)]\tLoss: 0.000045\n",
      "Train Epoch: 6 [22800/60000 (38%)]\tLoss: 0.000016\n",
      "Train Epoch: 6 [23400/60000 (39%)]\tLoss: 0.000005\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.000015\n",
      "Train Epoch: 6 [24600/60000 (41%)]\tLoss: 0.000171\n",
      "Train Epoch: 6 [25200/60000 (42%)]\tLoss: 0.000036\n",
      "Train Epoch: 6 [25800/60000 (43%)]\tLoss: 0.000015\n",
      "Train Epoch: 6 [26400/60000 (44%)]\tLoss: 0.000058\n",
      "Train Epoch: 6 [27000/60000 (45%)]\tLoss: 0.000033\n",
      "Train Epoch: 6 [27600/60000 (46%)]\tLoss: 0.001680\n",
      "Train Epoch: 6 [28200/60000 (47%)]\tLoss: 0.000041\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.000039\n",
      "Train Epoch: 6 [29400/60000 (49%)]\tLoss: 0.000005\n",
      "Validation set: Average loss: 0.0013, Accuracy: 5993/6000 (100%)\n",
      "Test set: Average loss: 0.0049, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.000186\n",
      "Train Epoch: 6 [30600/60000 (51%)]\tLoss: 0.000094\n",
      "Train Epoch: 6 [31200/60000 (52%)]\tLoss: 0.000031\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tLoss: 0.001735\n",
      "Train Epoch: 6 [32400/60000 (54%)]\tLoss: 0.000261\n",
      "Train Epoch: 6 [33000/60000 (55%)]\tLoss: 0.000004\n",
      "Train Epoch: 6 [33600/60000 (56%)]\tLoss: 0.000025\n",
      "Train Epoch: 6 [34200/60000 (57%)]\tLoss: 0.000272\n",
      "Train Epoch: 6 [34800/60000 (58%)]\tLoss: 0.000258\n",
      "Train Epoch: 6 [35400/60000 (59%)]\tLoss: 0.000073\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.000019\n",
      "Train Epoch: 6 [36600/60000 (61%)]\tLoss: 0.000015\n",
      "Train Epoch: 6 [37200/60000 (62%)]\tLoss: 0.000032\n",
      "Train Epoch: 6 [37800/60000 (63%)]\tLoss: 0.000006\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.000038\n",
      "Train Epoch: 6 [39000/60000 (65%)]\tLoss: 0.000230\n",
      "Train Epoch: 6 [39600/60000 (66%)]\tLoss: 0.000136\n",
      "Train Epoch: 6 [40200/60000 (67%)]\tLoss: 0.000025\n",
      "Train Epoch: 6 [40800/60000 (68%)]\tLoss: 0.000023\n",
      "Train Epoch: 6 [41400/60000 (69%)]\tLoss: 0.000026\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [42600/60000 (71%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [43200/60000 (72%)]\tLoss: 0.000287\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tLoss: 0.000008\n",
      "Train Epoch: 6 [44400/60000 (74%)]\tLoss: 0.000024\n",
      "Validation set: Average loss: 0.0012, Accuracy: 5993/6000 (100%)\n",
      "Test set: Average loss: 0.0050, Accuracy: 9950/10000 (100%)\n",
      "\n",
      "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 0.000223\n",
      "Train Epoch: 6 [45600/60000 (76%)]\tLoss: 0.000194\n",
      "Train Epoch: 6 [46200/60000 (77%)]\tLoss: 0.000021\n",
      "Train Epoch: 6 [46800/60000 (78%)]\tLoss: 0.000019\n",
      "Train Epoch: 6 [47400/60000 (79%)]\tLoss: 0.000072\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.000454\n",
      "Train Epoch: 6 [48600/60000 (81%)]\tLoss: 0.000038\n",
      "Train Epoch: 6 [49200/60000 (82%)]\tLoss: 0.001049\n",
      "Train Epoch: 6 [49800/60000 (83%)]\tLoss: 0.000018\n",
      "Train Epoch: 6 [50400/60000 (84%)]\tLoss: 0.000048\n",
      "Train Epoch: 6 [51000/60000 (85%)]\tLoss: 0.000302\n",
      "Train Epoch: 6 [51600/60000 (86%)]\tLoss: 0.000019\n",
      "Train Epoch: 6 [52200/60000 (87%)]\tLoss: 0.000010\n",
      "Train Epoch: 6 [52800/60000 (88%)]\tLoss: 0.000007\n",
      "Train Epoch: 6 [53400/60000 (89%)]\tLoss: 0.000026\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.000006\n",
      "Train Epoch: 6 [54600/60000 (91%)]\tLoss: 0.000016\n",
      "Train Epoch: 6 [55200/60000 (92%)]\tLoss: 0.000025\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [56400/60000 (94%)]\tLoss: 0.000007\n",
      "Train Epoch: 6 [57000/60000 (95%)]\tLoss: 0.000006\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.000109\n",
      "Train Epoch: 6 [58200/60000 (97%)]\tLoss: 0.000027\n",
      "Train Epoch: 6 [58800/60000 (98%)]\tLoss: 0.000124\n",
      "Train Epoch: 6 [59400/60000 (99%)]\tLoss: 0.000009\n",
      "Validation set: Average loss: 0.0011, Accuracy: 5992/6000 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0047, Accuracy: 9956/10000 (100%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000140\n",
      "Train Epoch: 7 [600/60000 (1%)]\tLoss: 0.000027\n",
      "Train Epoch: 7 [1200/60000 (2%)]\tLoss: 0.000004\n",
      "Train Epoch: 7 [1800/60000 (3%)]\tLoss: 0.000069\n",
      "Train Epoch: 7 [2400/60000 (4%)]\tLoss: 0.000003\n",
      "Train Epoch: 7 [3000/60000 (5%)]\tLoss: 0.000106\n",
      "Train Epoch: 7 [3600/60000 (6%)]\tLoss: 0.000073\n",
      "Train Epoch: 7 [4200/60000 (7%)]\tLoss: 0.000197\n",
      "Train Epoch: 7 [4800/60000 (8%)]\tLoss: 0.000046\n",
      "Train Epoch: 7 [5400/60000 (9%)]\tLoss: 0.000015\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.000185\n",
      "Train Epoch: 7 [6600/60000 (11%)]\tLoss: 0.000041\n",
      "Train Epoch: 7 [7200/60000 (12%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tLoss: 0.000004\n",
      "Train Epoch: 7 [8400/60000 (14%)]\tLoss: 0.000048\n",
      "Train Epoch: 7 [9000/60000 (15%)]\tLoss: 0.000011\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.000330\n",
      "Train Epoch: 7 [10200/60000 (17%)]\tLoss: 0.000008\n",
      "Train Epoch: 7 [10800/60000 (18%)]\tLoss: 0.000768\n",
      "Train Epoch: 7 [11400/60000 (19%)]\tLoss: 0.000032\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.000148\n",
      "Train Epoch: 7 [12600/60000 (21%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [13200/60000 (22%)]\tLoss: 0.000448\n",
      "Train Epoch: 7 [13800/60000 (23%)]\tLoss: 0.000169\n",
      "Train Epoch: 7 [14400/60000 (24%)]\tLoss: 0.000012\n",
      "Validation set: Average loss: 0.0009, Accuracy: 5994/6000 (100%)\n",
      "Test set: Average loss: 0.0049, Accuracy: 9954/10000 (100%)\n",
      "\n",
      "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 0.000015\n",
      "Train Epoch: 7 [15600/60000 (26%)]\tLoss: 0.000020\n",
      "Train Epoch: 7 [16200/60000 (27%)]\tLoss: 0.000009\n",
      "Train Epoch: 7 [16800/60000 (28%)]\tLoss: 0.000020\n",
      "Train Epoch: 7 [17400/60000 (29%)]\tLoss: 0.000009\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.000011\n",
      "Train Epoch: 7 [18600/60000 (31%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000145\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tLoss: 0.000003\n",
      "Train Epoch: 7 [20400/60000 (34%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [21000/60000 (35%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [21600/60000 (36%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [22200/60000 (37%)]\tLoss: 0.000036\n",
      "Train Epoch: 7 [22800/60000 (38%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [23400/60000 (39%)]\tLoss: 0.000030\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [24600/60000 (41%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [25200/60000 (42%)]\tLoss: 0.000017\n",
      "Train Epoch: 7 [25800/60000 (43%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [26400/60000 (44%)]\tLoss: 0.000021\n",
      "Train Epoch: 7 [27000/60000 (45%)]\tLoss: 0.000013\n",
      "Train Epoch: 7 [27600/60000 (46%)]\tLoss: 0.000009\n",
      "Train Epoch: 7 [28200/60000 (47%)]\tLoss: 0.000026\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.000018\n",
      "Train Epoch: 7 [29400/60000 (49%)]\tLoss: 0.000023\n",
      "Validation set: Average loss: 0.0009, Accuracy: 5993/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.000011\n",
      "Train Epoch: 7 [30600/60000 (51%)]\tLoss: 0.000129\n",
      "Train Epoch: 7 [31200/60000 (52%)]\tLoss: 0.000034\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tLoss: 0.000394\n",
      "Train Epoch: 7 [32400/60000 (54%)]\tLoss: 0.000025\n",
      "Train Epoch: 7 [33000/60000 (55%)]\tLoss: 0.000040\n",
      "Train Epoch: 7 [33600/60000 (56%)]\tLoss: 0.000035\n",
      "Train Epoch: 7 [34200/60000 (57%)]\tLoss: 0.000034\n",
      "Train Epoch: 7 [34800/60000 (58%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [35400/60000 (59%)]\tLoss: 0.000140\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.000021\n",
      "Train Epoch: 7 [36600/60000 (61%)]\tLoss: 0.000042\n",
      "Train Epoch: 7 [37200/60000 (62%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [37800/60000 (63%)]\tLoss: 0.000032\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000127\n",
      "Train Epoch: 7 [39000/60000 (65%)]\tLoss: 0.000091\n",
      "Train Epoch: 7 [39600/60000 (66%)]\tLoss: 0.000030\n",
      "Train Epoch: 7 [40200/60000 (67%)]\tLoss: 0.000060\n",
      "Train Epoch: 7 [40800/60000 (68%)]\tLoss: 0.000070\n",
      "Train Epoch: 7 [41400/60000 (69%)]\tLoss: 0.000048\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [42600/60000 (71%)]\tLoss: 0.000204\n",
      "Train Epoch: 7 [43200/60000 (72%)]\tLoss: 0.000035\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [44400/60000 (74%)]\tLoss: 0.000015\n",
      "Validation set: Average loss: 0.0008, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9953/10000 (100%)\n",
      "\n",
      "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 0.000077\n",
      "Train Epoch: 7 [45600/60000 (76%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [46200/60000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [46800/60000 (78%)]\tLoss: 0.000013\n",
      "Train Epoch: 7 [47400/60000 (79%)]\tLoss: 0.000037\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.000068\n",
      "Train Epoch: 7 [48600/60000 (81%)]\tLoss: 0.000029\n",
      "Train Epoch: 7 [49200/60000 (82%)]\tLoss: 0.000010\n",
      "Train Epoch: 7 [49800/60000 (83%)]\tLoss: 0.000019\n",
      "Train Epoch: 7 [50400/60000 (84%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [51000/60000 (85%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [51600/60000 (86%)]\tLoss: 0.000023\n",
      "Train Epoch: 7 [52200/60000 (87%)]\tLoss: 0.000049\n",
      "Train Epoch: 7 [52800/60000 (88%)]\tLoss: 0.000083\n",
      "Train Epoch: 7 [53400/60000 (89%)]\tLoss: 0.000047\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.000037\n",
      "Train Epoch: 7 [54600/60000 (91%)]\tLoss: 0.000388\n",
      "Train Epoch: 7 [55200/60000 (92%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tLoss: 0.000010\n",
      "Train Epoch: 7 [56400/60000 (94%)]\tLoss: 0.000029\n",
      "Train Epoch: 7 [57000/60000 (95%)]\tLoss: 0.000226\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.000077\n",
      "Train Epoch: 7 [58200/60000 (97%)]\tLoss: 0.000039\n",
      "Train Epoch: 7 [58800/60000 (98%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [59400/60000 (99%)]\tLoss: 0.000006\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Validation set: Average loss: 0.0009, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0052, Accuracy: 9945/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000044\n",
      "Train Epoch: 8 [600/60000 (1%)]\tLoss: 0.000088\n",
      "Train Epoch: 8 [1200/60000 (2%)]\tLoss: 0.000010\n",
      "Train Epoch: 8 [1800/60000 (3%)]\tLoss: 0.000023\n",
      "Train Epoch: 8 [2400/60000 (4%)]\tLoss: 0.000025\n",
      "Train Epoch: 8 [3000/60000 (5%)]\tLoss: 0.000007\n",
      "Train Epoch: 8 [3600/60000 (6%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [4200/60000 (7%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [4800/60000 (8%)]\tLoss: 0.000020\n",
      "Train Epoch: 8 [5400/60000 (9%)]\tLoss: 0.000014\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [6600/60000 (11%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [7200/60000 (12%)]\tLoss: 0.000023\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tLoss: 0.000009\n",
      "Train Epoch: 8 [8400/60000 (14%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [9000/60000 (15%)]\tLoss: 0.000057\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.000070\n",
      "Train Epoch: 8 [10200/60000 (17%)]\tLoss: 0.000020\n",
      "Train Epoch: 8 [10800/60000 (18%)]\tLoss: 0.000058\n",
      "Train Epoch: 8 [11400/60000 (19%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.000009\n",
      "Train Epoch: 8 [12600/60000 (21%)]\tLoss: 0.000120\n",
      "Train Epoch: 8 [13200/60000 (22%)]\tLoss: 0.000007\n",
      "Train Epoch: 8 [13800/60000 (23%)]\tLoss: 0.000020\n",
      "Train Epoch: 8 [14400/60000 (24%)]\tLoss: 0.000005\n",
      "Validation set: Average loss: 0.0008, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 9949/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [15600/60000 (26%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [16200/60000 (27%)]\tLoss: 0.000019\n",
      "Train Epoch: 8 [16800/60000 (28%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [17400/60000 (29%)]\tLoss: 0.000018\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.000040\n",
      "Train Epoch: 8 [18600/60000 (31%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [20400/60000 (34%)]\tLoss: 0.000004\n",
      "Train Epoch: 8 [21000/60000 (35%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [21600/60000 (36%)]\tLoss: 0.000018\n",
      "Train Epoch: 8 [22200/60000 (37%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [22800/60000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [23400/60000 (39%)]\tLoss: 0.000010\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [24600/60000 (41%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [25200/60000 (42%)]\tLoss: 0.000031\n",
      "Train Epoch: 8 [25800/60000 (43%)]\tLoss: 0.000018\n",
      "Train Epoch: 8 [26400/60000 (44%)]\tLoss: 0.000034\n",
      "Train Epoch: 8 [27000/60000 (45%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [27600/60000 (46%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [28200/60000 (47%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.000022\n",
      "Train Epoch: 8 [29400/60000 (49%)]\tLoss: 0.000000\n",
      "Validation set: Average loss: 0.0008, Accuracy: 5996/6000 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [30600/60000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [31200/60000 (52%)]\tLoss: 0.000037\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [32400/60000 (54%)]\tLoss: 0.000027\n",
      "Train Epoch: 8 [33000/60000 (55%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [33600/60000 (56%)]\tLoss: 0.000029\n",
      "Train Epoch: 8 [34200/60000 (57%)]\tLoss: 0.000052\n",
      "Train Epoch: 8 [34800/60000 (58%)]\tLoss: 0.000015\n",
      "Train Epoch: 8 [35400/60000 (59%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.000024\n",
      "Train Epoch: 8 [36600/60000 (61%)]\tLoss: 0.000034\n",
      "Train Epoch: 8 [37200/60000 (62%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [37800/60000 (63%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [39000/60000 (65%)]\tLoss: 0.000036\n",
      "Train Epoch: 8 [39600/60000 (66%)]\tLoss: 0.000029\n",
      "Train Epoch: 8 [40200/60000 (67%)]\tLoss: 0.000014\n",
      "Train Epoch: 8 [40800/60000 (68%)]\tLoss: 0.000020\n",
      "Train Epoch: 8 [41400/60000 (69%)]\tLoss: 0.000004\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.000073\n",
      "Train Epoch: 8 [42600/60000 (71%)]\tLoss: 0.000029\n",
      "Train Epoch: 8 [43200/60000 (72%)]\tLoss: 0.000032\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [44400/60000 (74%)]\tLoss: 0.000006\n",
      "Validation set: Average loss: 0.0007, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9949/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 0.000153\n",
      "Train Epoch: 8 [45600/60000 (76%)]\tLoss: 0.000020\n",
      "Train Epoch: 8 [46200/60000 (77%)]\tLoss: 0.000024\n",
      "Train Epoch: 8 [46800/60000 (78%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [47400/60000 (79%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.000043\n",
      "Train Epoch: 8 [48600/60000 (81%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [49200/60000 (82%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [49800/60000 (83%)]\tLoss: 0.000023\n",
      "Train Epoch: 8 [50400/60000 (84%)]\tLoss: 0.000022\n",
      "Train Epoch: 8 [51000/60000 (85%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [51600/60000 (86%)]\tLoss: 0.000007\n",
      "Train Epoch: 8 [52200/60000 (87%)]\tLoss: 0.000023\n",
      "Train Epoch: 8 [52800/60000 (88%)]\tLoss: 0.000026\n",
      "Train Epoch: 8 [53400/60000 (89%)]\tLoss: 0.000108\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.000033\n",
      "Train Epoch: 8 [54600/60000 (91%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [55200/60000 (92%)]\tLoss: 0.000012\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tLoss: 0.000004\n",
      "Train Epoch: 8 [56400/60000 (94%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [57000/60000 (95%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [58200/60000 (97%)]\tLoss: 0.000017\n",
      "Train Epoch: 8 [58800/60000 (98%)]\tLoss: 0.000045\n",
      "Train Epoch: 8 [59400/60000 (99%)]\tLoss: 0.000021\n",
      "Validation set: Average loss: 0.0007, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [600/60000 (1%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [1200/60000 (2%)]\tLoss: 0.000567\n",
      "Train Epoch: 9 [1800/60000 (3%)]\tLoss: 0.000040\n",
      "Train Epoch: 9 [2400/60000 (4%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [3000/60000 (5%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [3600/60000 (6%)]\tLoss: 0.000259\n",
      "Train Epoch: 9 [4200/60000 (7%)]\tLoss: 0.000027\n",
      "Train Epoch: 9 [4800/60000 (8%)]\tLoss: 0.000117\n",
      "Train Epoch: 9 [5400/60000 (9%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [6600/60000 (11%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [7200/60000 (12%)]\tLoss: 0.000012\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [8400/60000 (14%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [9000/60000 (15%)]\tLoss: 0.000021\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.000011\n",
      "Train Epoch: 9 [10200/60000 (17%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [10800/60000 (18%)]\tLoss: 0.000017\n",
      "Train Epoch: 9 [11400/60000 (19%)]\tLoss: 0.000086\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [12600/60000 (21%)]\tLoss: 0.000012\n",
      "Train Epoch: 9 [13200/60000 (22%)]\tLoss: 0.000018\n",
      "Train Epoch: 9 [13800/60000 (23%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [14400/60000 (24%)]\tLoss: 0.000028\n",
      "Validation set: Average loss: 0.0007, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [15600/60000 (26%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [16200/60000 (27%)]\tLoss: 0.000012\n",
      "Train Epoch: 9 [16800/60000 (28%)]\tLoss: 0.000324\n",
      "Train Epoch: 9 [17400/60000 (29%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.000010\n",
      "Train Epoch: 9 [18600/60000 (31%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tLoss: 0.000072\n",
      "Train Epoch: 9 [20400/60000 (34%)]\tLoss: 0.000026\n",
      "Train Epoch: 9 [21000/60000 (35%)]\tLoss: 0.000012\n",
      "Train Epoch: 9 [21600/60000 (36%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [22200/60000 (37%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [22800/60000 (38%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [23400/60000 (39%)]\tLoss: 0.000021\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.000352\n",
      "Train Epoch: 9 [24600/60000 (41%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [25200/60000 (42%)]\tLoss: 0.000025\n",
      "Train Epoch: 9 [25800/60000 (43%)]\tLoss: 0.000031\n",
      "Train Epoch: 9 [26400/60000 (44%)]\tLoss: 0.000027\n",
      "Train Epoch: 9 [27000/60000 (45%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [27600/60000 (46%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [28200/60000 (47%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [29400/60000 (49%)]\tLoss: 0.000018\n",
      "Validation set: Average loss: 0.0007, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9954/10000 (100%)\n",
      "\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.000049\n",
      "Train Epoch: 9 [30600/60000 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [31200/60000 (52%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [32400/60000 (54%)]\tLoss: 0.000010\n",
      "Train Epoch: 9 [33000/60000 (55%)]\tLoss: 0.000020\n",
      "Train Epoch: 9 [33600/60000 (56%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [34200/60000 (57%)]\tLoss: 0.000010\n",
      "Train Epoch: 9 [34800/60000 (58%)]\tLoss: 0.000033\n",
      "Train Epoch: 9 [35400/60000 (59%)]\tLoss: 0.000113\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.000022\n",
      "Train Epoch: 9 [36600/60000 (61%)]\tLoss: 0.000043\n",
      "Train Epoch: 9 [37200/60000 (62%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [37800/60000 (63%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000027\n",
      "Train Epoch: 9 [39000/60000 (65%)]\tLoss: 0.000159\n",
      "Train Epoch: 9 [39600/60000 (66%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [40200/60000 (67%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [40800/60000 (68%)]\tLoss: 0.000042\n",
      "Train Epoch: 9 [41400/60000 (69%)]\tLoss: 0.000224\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [42600/60000 (71%)]\tLoss: 0.000010\n",
      "Train Epoch: 9 [43200/60000 (72%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [44400/60000 (74%)]\tLoss: 0.000001\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 9949/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 0.000215\n",
      "Train Epoch: 9 [45600/60000 (76%)]\tLoss: 0.000013\n",
      "Train Epoch: 9 [46200/60000 (77%)]\tLoss: 0.000127\n",
      "Train Epoch: 9 [46800/60000 (78%)]\tLoss: 0.000052\n",
      "Train Epoch: 9 [47400/60000 (79%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.000011\n",
      "Train Epoch: 9 [48600/60000 (81%)]\tLoss: 0.000015\n",
      "Train Epoch: 9 [49200/60000 (82%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [49800/60000 (83%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [50400/60000 (84%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [51000/60000 (85%)]\tLoss: 0.000025\n",
      "Train Epoch: 9 [51600/60000 (86%)]\tLoss: 0.000021\n",
      "Train Epoch: 9 [52200/60000 (87%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [52800/60000 (88%)]\tLoss: 0.000020\n",
      "Train Epoch: 9 [53400/60000 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [54600/60000 (91%)]\tLoss: 0.000015\n",
      "Train Epoch: 9 [55200/60000 (92%)]\tLoss: 0.000027\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tLoss: 0.000087\n",
      "Train Epoch: 9 [56400/60000 (94%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [57000/60000 (95%)]\tLoss: 0.000017\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [58200/60000 (97%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [58800/60000 (98%)]\tLoss: 0.000021\n",
      "Train Epoch: 9 [59400/60000 (99%)]\tLoss: 0.000003\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [600/60000 (1%)]\tLoss: 0.000018\n",
      "Train Epoch: 10 [1200/60000 (2%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [1800/60000 (3%)]\tLoss: 0.000012\n",
      "Train Epoch: 10 [2400/60000 (4%)]\tLoss: 0.000009\n",
      "Train Epoch: 10 [3000/60000 (5%)]\tLoss: 0.000085\n",
      "Train Epoch: 10 [3600/60000 (6%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [4200/60000 (7%)]\tLoss: 0.000017\n",
      "Train Epoch: 10 [4800/60000 (8%)]\tLoss: 0.000007\n",
      "Train Epoch: 10 [5400/60000 (9%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 0.000027\n",
      "Train Epoch: 10 [6600/60000 (11%)]\tLoss: 0.000048\n",
      "Train Epoch: 10 [7200/60000 (12%)]\tLoss: 0.000010\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tLoss: 0.000009\n",
      "Train Epoch: 10 [8400/60000 (14%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [9000/60000 (15%)]\tLoss: 0.000011\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [10200/60000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [10800/60000 (18%)]\tLoss: 0.000030\n",
      "Train Epoch: 10 [11400/60000 (19%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 0.000017\n",
      "Train Epoch: 10 [12600/60000 (21%)]\tLoss: 0.000051\n",
      "Train Epoch: 10 [13200/60000 (22%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [13800/60000 (23%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [14400/60000 (24%)]\tLoss: 0.000001\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 9948/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 0.000008\n",
      "Train Epoch: 10 [15600/60000 (26%)]\tLoss: 0.000012\n",
      "Train Epoch: 10 [16200/60000 (27%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [16800/60000 (28%)]\tLoss: 0.000049\n",
      "Train Epoch: 10 [17400/60000 (29%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [18600/60000 (31%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000033\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tLoss: 0.000008\n",
      "Train Epoch: 10 [20400/60000 (34%)]\tLoss: 0.000126\n",
      "Train Epoch: 10 [21000/60000 (35%)]\tLoss: 0.000046\n",
      "Train Epoch: 10 [21600/60000 (36%)]\tLoss: 0.000020\n",
      "Train Epoch: 10 [22200/60000 (37%)]\tLoss: 0.000034\n",
      "Train Epoch: 10 [22800/60000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [23400/60000 (39%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.000014\n",
      "Train Epoch: 10 [24600/60000 (41%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [25200/60000 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [25800/60000 (43%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [26400/60000 (44%)]\tLoss: 0.000025\n",
      "Train Epoch: 10 [27000/60000 (45%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [27600/60000 (46%)]\tLoss: 0.000055\n",
      "Train Epoch: 10 [28200/60000 (47%)]\tLoss: 0.000007\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [29400/60000 (49%)]\tLoss: 0.000007\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9952/10000 (100%)\n",
      "\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.000011\n",
      "Train Epoch: 10 [30600/60000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [31200/60000 (52%)]\tLoss: 0.000011\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [32400/60000 (54%)]\tLoss: 0.000014\n",
      "Train Epoch: 10 [33000/60000 (55%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [33600/60000 (56%)]\tLoss: 0.000016\n",
      "Train Epoch: 10 [34200/60000 (57%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [34800/60000 (58%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [35400/60000 (59%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.000288\n",
      "Train Epoch: 10 [36600/60000 (61%)]\tLoss: 0.000020\n",
      "Train Epoch: 10 [37200/60000 (62%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [37800/60000 (63%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [39000/60000 (65%)]\tLoss: 0.000014\n",
      "Train Epoch: 10 [39600/60000 (66%)]\tLoss: 0.000008\n",
      "Train Epoch: 10 [40200/60000 (67%)]\tLoss: 0.000014\n",
      "Train Epoch: 10 [40800/60000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [41400/60000 (69%)]\tLoss: 0.000007\n",
      "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [42600/60000 (71%)]\tLoss: 0.000017\n",
      "Train Epoch: 10 [43200/60000 (72%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tLoss: 0.000579\n",
      "Train Epoch: 10 [44400/60000 (74%)]\tLoss: 0.000004\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 9951/10000 (100%)\n",
      "\n",
      "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [45600/60000 (76%)]\tLoss: 0.000011\n",
      "Train Epoch: 10 [46200/60000 (77%)]\tLoss: 0.000009\n",
      "Train Epoch: 10 [46800/60000 (78%)]\tLoss: 0.001454\n",
      "Train Epoch: 10 [47400/60000 (79%)]\tLoss: 0.000013\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [48600/60000 (81%)]\tLoss: 0.000010\n",
      "Train Epoch: 10 [49200/60000 (82%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [49800/60000 (83%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [50400/60000 (84%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [51000/60000 (85%)]\tLoss: 0.000011\n",
      "Train Epoch: 10 [51600/60000 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [52200/60000 (87%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [52800/60000 (88%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [53400/60000 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [54600/60000 (91%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [55200/60000 (92%)]\tLoss: 0.000100\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [56400/60000 (94%)]\tLoss: 0.000007\n",
      "Train Epoch: 10 [57000/60000 (95%)]\tLoss: 0.000002\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.000025\n",
      "Train Epoch: 10 [58200/60000 (97%)]\tLoss: 0.000034\n",
      "Train Epoch: 10 [58800/60000 (98%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [59400/60000 (99%)]\tLoss: 0.000001\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Test set: Average loss: 0.0047, Accuracy: 9953/10000 (100%)\n",
      "Validation set: Average loss: 0.0006, Accuracy: 5996/6000 (100%)\n",
      "Train set: Average loss: 0.0003, Accuracy: 59984/60000 (100%)\n"
     ]
    }
   ],
   "source": [
    "### Call training functions \n",
    "TrainLoss = []; TrainL_every = 1; TrN=0; TrD=0\n",
    "ValLoss = []; ValAcc = []; VN=0;VD=0\n",
    "TestLoss = []; TestAcc = []; TN=0; TD=0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch,TrainLoss,NUMSTATSPEREPOCH,TrainL_every)\n",
    "    scheduler.step(val_loss)\n",
    "# get final training and testing stats\n",
    "ftest_loss, ftest_acc,TN,TD = getLossAccND('Test',TESTLOADER)\n",
    "val_loss, val_acc,VN,VD = getLossAccND('Validation',VALLOADER)\n",
    "ftrain_loss, ftrain_acc,TrN,TrD = getLossAccND('Train',TRAINLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network classes\n",
    "#capsnet = CapsNet(3, 10)\n",
    "#reconstructionnet = ReconstructionNet(16, 10)\n",
    "#model = CapsNetWithReconstruction(capsnet, reconstructionnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f776dae9850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 59984/60000 (99.97%)\n",
      "Final Training Loss: 0.00031\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAElCAYAAACxnHbGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f776df9c290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 5996/6000 (99.93%)\n",
      "Final Validation Loss: 0.00059\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAElCAYAAACxnHbGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZlZKTmgzMyslBxQZmZWSv8fwhQ4a8os3k8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7766f7cf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 9953/10000 (99.51%)\n",
      "Final Test Loss: 0.00466\n"
     ]
    }
   ],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.plot(TrainLoss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "print 'Final Training Accuracy: %d/%d (%.2f%%)' % (TrN,TrD,ftrain_acc)\n",
    "print 'Final Training Loss: %.5f' % (ftrain_loss)\n",
    "\n",
    "statsIdx = range(len(ValAcc))\n",
    "statsIdx = [float(c)/NUMSTATSPEREPOCH for c in statsIdx]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(statsIdx,ValLoss, 'b-')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(statsIdx,ValAcc, 'r-')\n",
    "ax2.set_ylabel('Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()\n",
    "plt.title('Validation Loss and Accuracy')\n",
    "plt.show()\n",
    "print 'Final Validation Accuracy: %d/%d (%.2f%%)' % (VN,VD,ValAcc[-1])\n",
    "print 'Final Validation Loss: %.5f' % (ValLoss[-1])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(statsIdx,TestLoss, 'b-')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(statsIdx,TestAcc, 'r-')\n",
    "ax2.set_ylabel('Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()\n",
    "plt.title('Test Loss and Accuracy')\n",
    "plt.show()\n",
    "print 'Final Test Accuracy: %d/%d (%.2f%%)' % (TN,TD,TestAcc[-1])\n",
    "print 'Final Test Loss: %.5f' % (TestLoss[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
